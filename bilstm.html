
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bidireccional LSTM Para Análisis de Sentimiento en texto &#8212; Parcial práctico 2 - Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'bilstm';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="LSTM: Long Short-Term Memory" href="lstm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/uninorte.jpg" class="logo__image only-light" alt="Parcial práctico 2 - Deep Learning - Home"/>
    <script>document.write(`<img src="_static/uninorte.jpg" class="logo__image only-dark" alt="Parcial práctico 2 - Deep Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Parcial práctico 2 - Deep Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="eda.html"><strong>Análisis Exploratorio de Texto para Procesamiento del Lenguaje Natural (NLP)</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html"><strong>RNN: Red Neuronal Recurrente</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html"><strong>LSTM: Long Short-Term Memory</strong></a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bidireccional LSTM Para Análisis de Sentimiento en texto</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/kmarcela11/Parcial2_DeepLearning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/kmarcela11/Parcial2_DeepLearning/issues/new?title=Issue%20on%20page%20%2Fbilstm.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/bilstm.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bidireccional LSTM Para Análisis de Sentimiento en texto</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#librerias-y-paquetes-a-utilizar">Librerías y paquetes a utilizar</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-bidireccional-long-short-term-memory-lstm">Modelo Bidireccional Long Short Term Memory (LSTM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cargue-de-datos">Cargue de datos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametros-del-modelo">Parámetros del modelo</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hiperparametrizacion-para-bilstm">Hiperparametrización para BiLSTM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-final-con-los-mejores-parametros">Modelo final con los mejores parámetros</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metricas-de-prediccion">Métricas de Predicción</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bidireccional-lstm-para-analisis-de-sentimiento-en-texto">
<h1>Bidireccional LSTM Para Análisis de Sentimiento en texto<a class="headerlink" href="#bidireccional-lstm-para-analisis-de-sentimiento-en-texto" title="Link to this heading">#</a></h1>
<section id="librerias-y-paquetes-a-utilizar">
<h2>Librerías y paquetes a utilizar<a class="headerlink" href="#librerias-y-paquetes-a-utilizar" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;progress-bar&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Doc2Vec</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">keras_preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">gensim.models.doc2vec</span> <span class="kn">import</span> <span class="n">TaggedDocument</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Bidirectional</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">mpimg</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ParameterGrid</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="modelo-bidireccional-long-short-term-memory-lstm">
<h2>Modelo Bidireccional Long Short Term Memory (LSTM)<a class="headerlink" href="#modelo-bidireccional-long-short-term-memory-lstm" title="Link to this heading">#</a></h2>
<div class="admonition-que-es-bidireccional-lstm admonition">
<p class="admonition-title">¿Qué es Bidireccional LSTM?</p>
<p>Un modelo Bidireccional LSTM (BiLSTM) es una extensión de la arquitectura Long Short-Term Memory que permite al modelo procesar secuencias de datos en dos direcciones simultáneamente: desde el pasado hacia el futuro y viceversa. Mientras que un LSTM convencional solo capta la información de forma unidireccional (por ejemplo, de izquierda a derecha en una oración), un BiLSTM incorpora una segunda capa LSTM que recorre la secuencia en sentido inverso. Esto permite al modelo capturar tanto el contexto anterior como el posterior a cada palabra, lo cual resulta especialmente útil en tareas de procesamiento de lenguaje natural donde el significado de una palabra o frase puede depender de su entorno completo.</p>
</div>
<p><strong>¿Cómo funciona BiLSTM</strong></p>
<p>El modelo Bidireccional LSTM (BiLSTM) funciona extendiendo la arquitectura tradicional de una red LSTM para que procese las secuencias de datos en dos direcciones simultáneamente, en lugar de una sola:</p>
<ol class="arabic simple">
<li><p>Dos capas LSTM paralelas: El BiLSTM consta de dos redes LSTM independientes:</p>
<ul class="simple">
<li><p>Una LSTM que procesa la secuencia en orden normal (de izquierda a derecha).</p></li>
<li><p>Otra LSTM que procesa la misma secuencia en orden inverso (de derecha a izquierda).</p></li>
</ul>
</li>
<li><p>Entrada compartida, direcciones opuestas: Ambas redes reciben la misma secuencia de entrada (por ejemplo, una oración), pero cada una la recorre en sentido opuesto. Esto permite que una red capte el contexto previo y la otra el contexto posterior a cada elemento.</p></li>
<li><p>Concatenación de salidas: En cada posición de la secuencia (cada palabra), el BiLSTM concatena o combina las salidas generadas por ambas direcciones. Esto genera una representación enriquecida que considera información tanto pasada como futura.</p></li>
<li><p>Capas posteriores: Esta salida bidireccional se puede conectar a otras capas densas, de clasificación, de atención, etc., dependiendo del objetivo del modelo.</p></li>
</ol>
<p>En el procesamiento de lenguaje natural (NLP) se pueden implementar redes neuronales como LSTM para <code class="docutils literal notranslate"><span class="pre">análisis</span> <span class="pre">de</span> <span class="pre">sentimiento</span></code>. En éste último LSTM se utiliza para clasificar sentimientos en un texto, como en el caso de las reseñas de productos (positivas o negativas). El modelo puede entender las dependencias entre las palabras para decidir si el sentimiento general del texto es positivo o negativo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;C:/Users/henry/Documents/jbook/project_NLP1/images/BiLSTM.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Arquitectura BiLSTM&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ba101b77483ee88eeb438b90bc9bbf2fd223b51ac4dd723da861bdde44791fa4.png" src="_images/ba101b77483ee88eeb438b90bc9bbf2fd223b51ac4dd723da861bdde44791fa4.png" />
</div>
</div>
<section id="cargue-de-datos">
<h3>Cargue de datos<a class="headerlink" href="#cargue-de-datos" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">datapath</span> <span class="o">=</span> <span class="s1">&#39;C:/Users/henry/Documents/jbook/project_NLP1/datos/&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">datapath</span> <span class="o">+</span> <span class="s1">&#39;to_models.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># Asegúrate de que el texto sea una cadena</span>
        <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>  <span class="c1"># Corregido &quot;lowwer&quot; a &quot;lower&quot;</span>
    <span class="k">return</span> <span class="n">tokens</span>

<span class="n">train_tagged</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">TaggedDocument</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">tokenize</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">),</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">label</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_tagged</span>  <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">TaggedDocument</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">tokenize</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">),</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">label</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replace NaN values in the &#39;message&#39; column with an empty string</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1"># Process the &#39;message&#39; column to split words and count unique words</span>
<span class="n">all_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">())))</span>
<span class="n">word_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">all_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total de palabras únicas: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">word_counts</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total de palabras únicas: 27217
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_features</span> <span class="o">=</span> <span class="mi">6000</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    10998.000000
mean        10.717494
std         12.533056
min          0.000000
25%          4.000000
50%          7.000000
75%         12.000000
max        243.000000
Name: message, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_features</span> <span class="o">=</span> <span class="mi">5000</span>  
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">50</span> 

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="s1">&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[$$^_`{|}~&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found </span><span class="si">%s</span><span class="s1"> unique tokens.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of data tensor:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 10998 unique tokens.
Shape of data tensor: (10998, 100)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([TaggedDocument(words=[&#39;hahahaha&#39;, &#39;mogambo&#39;, &#39;khush&#39;], tags=[&#39;Positivo&#39;]),
       TaggedDocument(words=[&#39;inquiry&#39;, &#39;chahye&#39;], tags=[&#39;Positivo&#39;]),
       TaggedDocument(words=[&#39;wese&#39;, &#39;twitter&#39;, &#39;nazar&#39;, &#39;twitter&#39;, &#39;ronaq&#39;, &#39;dobala&#39;], tags=[&#39;Positivo&#39;]),
       ...,
       TaggedDocument(words=[&#39;baron&#39;, &#39;hsla&#39;, &#39;afzai&#39;, &#39;hee&#39;, &#39;hamarey&#39;, &#39;hey&#39;], tags=[&#39;Positivo&#39;]),
       TaggedDocument(words=[&#39;fia&#39;, &#39;dar&#39;, &#39;dr&#39;, &#39;asim&#39;, &#39;case&#39;, &#39;zidha&#39;, &#39;big&#39;, &#39;musharf&#39;, &#39;article&#39;, &#39;afsos&#39;, &#39;sahaft&#39;], tags=[&#39;Negativo&#39;]),
       TaggedDocument(words=[&#39;adeel&#39;, &#39;boss&#39;, &#39;pyar&#39;, &#39;bhara&#39;], tags=[&#39;Positivo&#39;])],
      dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span> <span class="o">=</span> <span class="n">Doc2Vec</span><span class="p">(</span><span class="n">dm</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dm_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.065</span><span class="p">,</span> <span class="n">min_alpha</span><span class="o">=</span><span class="mf">0.065</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">d2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">shuffle</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span><span class="p">)]),</span> <span class="n">total_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2v_model</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-=</span> <span class="mf">0.002</span>
    <span class="n">d2v_model</span><span class="o">.</span><span class="n">min_alpha</span> <span class="o">=</span> <span class="n">d2v_model</span><span class="o">.</span><span class="n">alpha</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 3738752.44it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 4789912.59it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1085657.15it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1332520.37it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1752540.21it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1803151.07it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 607241.96it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 2863021.69it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1730838.96it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1267569.61it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 2347870.88it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1391563.71it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 3344646.66it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 2196647.81it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 1010031.11it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 4197166.35it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 4402993.27it/s]
100%|██████████| 8798/8798 [00:00&lt;?, ?it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 8798637.72it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 562017.19it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 2446885.92it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 554667.69it/s]
100%|██████████| 8798/8798 [00:00&lt;00:00, 546510.57it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: total: 35.2 s
Wall time: 38.1 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">d2v_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Doc2Vec&lt;dm/m,d20,n5,w8,s0.001,t12&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">num_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>23871
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span>
<span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;noqsan&#39;, &#39;lapait&#39;, &#39;zarye&#39;, &#39;westren&#39;, &#39;chatpati&#39;, &#39;daffinetly&#39;, &#39;boldiya&#39;, &#39;paktoon&#39;, &#39;nalaqe&#39;, &#39;sahaft&#39;]
</pre></div>
</div>
</div>
</div>
<p>A continuación se entrenan los embeddings del modelo para poder realizar la hiperparametrización.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">dv</span><span class="p">)):</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">d2v_model</span><span class="o">.</span><span class="n">dv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">vec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1000</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">dv</span><span class="p">)</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vec</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
KeyedVectors&lt;vector_size=20, 2 keys&gt;
[-0.44807473 -2.704889   -7.7218657   2.997391   -2.0672789  -0.2319688
 -5.9688478   0.21713771 -8.400917    1.0867647   0.4456104   1.5249355
 -1.6133018   2.098417   -1.6664804  -7.8435726  -1.793396   -0.18616118
 -6.767064   -1.1986817 ]
1
KeyedVectors&lt;vector_size=20, 2 keys&gt;
[-2.934743    2.3148718   0.97134763  1.0171957   4.531644   -3.0377693
 -3.4410603  -4.5871058   4.575576   -3.4638448   4.573713    3.2726018
 -2.846714   -6.324047   -1.5647299   3.6651394   0.3148525   1.124608
 -0.54153943 -1.2454855 ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pakistan&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;donoon&#39;, 0.7961616516113281),
 (&#39;kamzarf&#39;, 0.7950690388679504),
 (&#39;sauda&#39;, 0.7943793535232544),
 (&#39;bhaagne&#39;, 0.7756441235542297),
 (&#39;tarki&#39;, 0.7728510499000549),
 (&#39;kitnein&#39;, 0.7670255303382874),
 (&#39;aajay&#39;, 0.7602293491363525),
 (&#39;kasai&#39;, 0.7599718570709229),
 (&#39;gadaar&#39;, 0.7580880522727966),
 (&#39;bhater&#39;, 0.7569281458854675)]
</pre></div>
</div>
</div>
</div>
</section>
<section id="parametros-del-modelo">
<h3>Parámetros del modelo<a class="headerlink" href="#parametros-del-modelo" title="Link to this heading">#</a></h3>
<p>En la ejecución de la arquitectura de la red BiLSTM se implementan parámetros que ajustan las predicciones. A continuación se muestra una lista y descripción de los mismos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">param_grid_lstm = {</span>
<span class="sd">    &#39;lstm_units&#39;: [64, 128, 256],</span>
<span class="sd">    &#39;num_lstm_layers&#39;: [1, 2, 3],</span>
<span class="sd">    &#39;dropout_rate&#39;: [0.2, 0.3, 0.5],</span>
<span class="sd">    &#39;learning_rate&#39;: [0.001, 0.005, 0.01],</span>
<span class="sd">    &#39;batch_size&#39;: [32, 64, 128],</span>
<span class="sd">    &#39;sequence_length&#39;: [50, 100, 200],</span>
<span class="sd">    &#39;epochs&#39;: [20, 50, 100],</span>
<span class="sd">    &#39;embedding_dim&#39;: [100, 200, 300]</span>
<span class="sd">}</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;\nparam_grid_lstm = {\n    &#39;lstm_units&#39;: [64, 128, 256],\n    &#39;num_lstm_layers&#39;: [1, 2, 3],\n    &#39;dropout_rate&#39;: [0.2, 0.3, 0.5],\n    &#39;learning_rate&#39;: [0.001, 0.005, 0.01],\n    &#39;batch_size&#39;: [32, 64, 128],\n    &#39;sequence_length&#39;: [50, 100, 200],\n    &#39;epochs&#39;: [20, 50, 100],\n    &#39;embedding_dim&#39;: [100, 200, 300]\n}\n&quot;
</pre></div>
</div>
</div>
</div>
<div class="admonition-lstm-units admonition">
<p class="admonition-title"><strong>lstm_units</strong></p>
<p>Representa el número de unidades (neuronas) en la capa LSTM. Controla la capacidad del modelo para aprender patrones y memorizar dependencias en los datos secuenciales.</p>
<ul class="simple">
<li><p>Más unidades: El modelo tendrá más capacidad para aprender patrones complejos, pero puede volverse más lento y propenso a sobreajuste.</p></li>
<li><p>Menos Unidades: El modelo puede ser más rápido, pero tendrá menor capacidad para aprender patrones complejos.</p></li>
</ul>
</div>
<p>Valores posibles: <code class="docutils literal notranslate"><span class="pre">[64,</span> <span class="pre">128,</span> <span class="pre">256]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">**num_lstm_layers**</span></code> : Indica el número de capas LSTM apiladas en la red. y tiene como valores en esta hiperparametrización = <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code></p>
<div class="admonition-droptout-rate admonition">
<p class="admonition-title"><strong>Droptout_rate</strong></p>
<p>Es la fracción de nodos que se desconectan aleatoriamente durante el entrenamiento para evitar el sobreajuste.</p>
<ul class="simple">
<li><p>Valores posibles: <code class="docutils literal notranslate"><span class="pre">[0.2,</span> <span class="pre">0.3,</span> <span class="pre">0.5]</span></code></p></li>
</ul>
</div>
<p><strong>Learning_Rate</strong>: La tasa de aprendizaje controla la magnitud de las actualizaciones en los pesos del modelo durante el entrenamiento. Entre mayor sea, el modelo puede aprender más rápido, pero corre el riesgo de no converger o saltarse el mínimo óptimo. Mientas que si es más baja, el modelo se entrenaría más lentamente y tiene menos probabilidades de saltarse mínimos locales, pero podría quedarse atrapado en un mínimo subóptimo si es demasiado baja.</p>
<ul class="simple">
<li><p>Valores posibles: <code class="docutils literal notranslate"><span class="pre">[0.001,</span> <span class="pre">0.005,</span> <span class="pre">0.01]</span></code></p></li>
</ul>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Batch_size</span></code></strong> = El tamaño del lote de datos que se usa para cada paso de entrenamiento. Tamaños pequeños de batch ayudan a una mejor generalización, pero mayor tiempo de entrenamiento y más ruido en la actualización de los pesos, mientras que tamaños grandes de batch generan menos ruido, pero puede ser más propenso al sobreajuste y podría no generalizar tan bien. Valores posibles = <code class="docutils literal notranslate"><span class="pre">[32,</span> <span class="pre">64,</span> <span class="pre">128]</span></code></p></li>
</ul>
<div class="admonition-sequence-length admonition">
<p class="admonition-title">Sequence_length</p>
<p>La longitud máxima de las secuencias de texto que se alimentan a la red. Los textos más largos se truncarán y los más cortos se rellenarán con ceros. Si se elige una longitud de secuencia más larga, se pierde más información de los mensajes largos, pero los mensajes cortos se mantienen completos.</p>
<ul class="simple">
<li><p>Valores posibles = [50, 100, 200]</p></li>
</ul>
</div>
<div class="admonition-embedding-dim admonition">
<p class="admonition-title">Embedding_dim</p>
<p>La dimensión de los vectores de palabras en la capa de embeddings. Define el tamaño de los vectores que representan las palabras. Vectores pequeños tienen menos capacidad para capturar significados semánticos ricos, pero más rápido de entrenar. y vectores grandes tienen más capacidad para capturar semántica rica, pero más costoso computacionalmente.</p>
<ul class="simple">
<li><p>valores posibles = [100, 200, 300]</p></li>
</ul>
</div>
</section>
</section>
<section id="hiperparametrizacion-para-bilstm">
<h2>Hiperparametrización para BiLSTM<a class="headerlink" href="#hiperparametrizacion-para-bilstm" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>

<span class="c1">#dImensiones</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
(10998, 100)
8798
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(8798, 3)
(2200, 3)
</pre></div>
</div>
</div>
</div>
<p><strong>Parámetros a optimizar</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid_lstm</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;lstm_units&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span>
    <span class="s1">&#39;num_lstm_layers&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="s1">&#39;dropout_rate&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="s1">&#39;sequence_length&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>
    <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
    <span class="s1">&#39;embedding_dim&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">ParameterGrid</span><span class="p">(</span><span class="n">param_grid_lstm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Combinaciones</span>
<span class="k">for</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">:</span>
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;sequence_length&#39;</span><span class="p">]</span> <span class="c1">#Del conjunto de parámetros</span>
    <span class="c1"># Replace NaN values in the &#39;message&#39; column with an empty string</span>
    <span class="n">train</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">test</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>

    <span class="c1"># test</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>
    <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>  
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>  

    <span class="c1">#modelo LSTM</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="c1">#Capa de embedding</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">))</span>
    <span class="c1"># Capa LSTM</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;lstm_units&#39;</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;dropout_rate&#39;</span><span class="p">]))</span>
    <span class="c1">#Capa densa para la salida</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>  
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Entrenando modelo con parámetros: </span><span class="si">{</span><span class="n">params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

    <span class="c1"># modelo</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">],</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test Loss: </span><span class="si">{</span><span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test Accuracy: </span><span class="si">{</span><span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_3 (Embedding)     (None, 50, 50)            1360900   
                                                                 
 bidirectional_3 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_3 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 7s 19ms/step - loss: 0.5995 - accuracy: 0.6660 - val_loss: 0.5227 - val_accuracy: 0.7239
Epoch 2/20
220/220 [==============================] - 3s 15ms/step - loss: 0.3870 - accuracy: 0.8278 - val_loss: 0.5275 - val_accuracy: 0.7494
Epoch 3/20
220/220 [==============================] - 3s 15ms/step - loss: 0.2762 - accuracy: 0.8828 - val_loss: 0.5979 - val_accuracy: 0.7403
Epoch 4/20
220/220 [==============================] - 6s 26ms/step - loss: 0.2176 - accuracy: 0.9133 - val_loss: 0.7189 - val_accuracy: 0.7409
Epoch 5/20
220/220 [==============================] - 5s 23ms/step - loss: 0.1725 - accuracy: 0.9352 - val_loss: 0.9008 - val_accuracy: 0.7227
Epoch 6/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1412 - accuracy: 0.9460 - val_loss: 0.8685 - val_accuracy: 0.7420
Epoch 7/20
220/220 [==============================] - 4s 17ms/step - loss: 0.1111 - accuracy: 0.9584 - val_loss: 1.0593 - val_accuracy: 0.7341
Epoch 8/20
220/220 [==============================] - 3s 15ms/step - loss: 0.0940 - accuracy: 0.9642 - val_loss: 1.1851 - val_accuracy: 0.7415
Epoch 9/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0750 - accuracy: 0.9723 - val_loss: 1.2687 - val_accuracy: 0.7398
Epoch 10/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0665 - accuracy: 0.9751 - val_loss: 1.3762 - val_accuracy: 0.7364
Epoch 11/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0568 - accuracy: 0.9797 - val_loss: 1.3991 - val_accuracy: 0.7369
Epoch 12/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0521 - accuracy: 0.9815 - val_loss: 1.5909 - val_accuracy: 0.7341
Epoch 13/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0491 - accuracy: 0.9807 - val_loss: 1.5780 - val_accuracy: 0.7318
Epoch 14/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0464 - accuracy: 0.9828 - val_loss: 1.5208 - val_accuracy: 0.7330
Epoch 15/20
220/220 [==============================] - 3s 15ms/step - loss: 0.0471 - accuracy: 0.9822 - val_loss: 1.6331 - val_accuracy: 0.7347
Epoch 16/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0437 - accuracy: 0.9820 - val_loss: 1.6822 - val_accuracy: 0.7295
Epoch 17/20
220/220 [==============================] - 3s 15ms/step - loss: 0.0390 - accuracy: 0.9844 - val_loss: 1.8528 - val_accuracy: 0.7312
Epoch 18/20
220/220 [==============================] - 8s 35ms/step - loss: 0.0362 - accuracy: 0.9847 - val_loss: 1.9662 - val_accuracy: 0.7312
Epoch 19/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0344 - accuracy: 0.9852 - val_loss: 2.0635 - val_accuracy: 0.7301
Epoch 20/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0335 - accuracy: 0.9855 - val_loss: 2.1835 - val_accuracy: 0.7352
69/69 [==============================] - 4s 21ms/step - loss: 1.9728 - accuracy: 0.7445
Test Loss: 1.9727901220321655
Test Accuracy: 0.7445454597473145
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_4 (Embedding)     (None, 100, 50)           1360900   
                                                                 
 bidirectional_4 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_4 (Dropout)         (None, 128)               0         
                                                                 
 dense_4 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 8s 26ms/step - loss: 0.5957 - accuracy: 0.6698 - val_loss: 0.5426 - val_accuracy: 0.7210
Epoch 2/20
220/220 [==============================] - 5s 21ms/step - loss: 0.3894 - accuracy: 0.8291 - val_loss: 0.5241 - val_accuracy: 0.7477
Epoch 3/20
220/220 [==============================] - 5s 21ms/step - loss: 0.2812 - accuracy: 0.8811 - val_loss: 0.6144 - val_accuracy: 0.7523
Epoch 4/20
220/220 [==============================] - 6s 25ms/step - loss: 0.2251 - accuracy: 0.9109 - val_loss: 0.7143 - val_accuracy: 0.7403
Epoch 5/20
220/220 [==============================] - 5s 22ms/step - loss: 0.1780 - accuracy: 0.9292 - val_loss: 0.7948 - val_accuracy: 0.7426
Epoch 6/20
220/220 [==============================] - 4s 20ms/step - loss: 0.1487 - accuracy: 0.9420 - val_loss: 0.8725 - val_accuracy: 0.7409
Epoch 7/20
220/220 [==============================] - 4s 20ms/step - loss: 0.1245 - accuracy: 0.9544 - val_loss: 0.9709 - val_accuracy: 0.7335
Epoch 8/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1096 - accuracy: 0.9598 - val_loss: 1.0390 - val_accuracy: 0.7415
Epoch 9/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0905 - accuracy: 0.9655 - val_loss: 1.2273 - val_accuracy: 0.7426
Epoch 10/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0774 - accuracy: 0.9727 - val_loss: 1.2469 - val_accuracy: 0.7312
Epoch 11/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0669 - accuracy: 0.9767 - val_loss: 1.4839 - val_accuracy: 0.7284
Epoch 12/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0609 - accuracy: 0.9783 - val_loss: 1.3565 - val_accuracy: 0.7261
Epoch 13/20
220/220 [==============================] - 6s 27ms/step - loss: 0.0543 - accuracy: 0.9805 - val_loss: 1.5966 - val_accuracy: 0.7375
Epoch 14/20
220/220 [==============================] - 6s 26ms/step - loss: 0.0516 - accuracy: 0.9805 - val_loss: 1.6236 - val_accuracy: 0.7312
Epoch 15/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0472 - accuracy: 0.9827 - val_loss: 1.6329 - val_accuracy: 0.7301
Epoch 16/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0467 - accuracy: 0.9824 - val_loss: 1.6300 - val_accuracy: 0.7250
Epoch 17/20
220/220 [==============================] - 5s 22ms/step - loss: 0.0482 - accuracy: 0.9821 - val_loss: 1.7407 - val_accuracy: 0.7341
Epoch 18/20
220/220 [==============================] - 5s 21ms/step - loss: 0.0445 - accuracy: 0.9821 - val_loss: 1.7684 - val_accuracy: 0.7267
Epoch 19/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0403 - accuracy: 0.9837 - val_loss: 1.8131 - val_accuracy: 0.7295
Epoch 20/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0376 - accuracy: 0.9844 - val_loss: 1.9078 - val_accuracy: 0.7307
69/69 [==============================] - 1s 12ms/step - loss: 1.7781 - accuracy: 0.7400
Test Loss: 1.7781118154525757
Test Accuracy: 0.7400000095367432
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_5&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_5 (Embedding)     (None, 200, 50)           1360900   
                                                                 
 bidirectional_5 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_5 (Dropout)         (None, 128)               0         
                                                                 
 dense_5 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 9s 34ms/step - loss: 0.6081 - accuracy: 0.6540 - val_loss: 0.5335 - val_accuracy: 0.7250
Epoch 2/20
220/220 [==============================] - 7s 30ms/step - loss: 0.3878 - accuracy: 0.8323 - val_loss: 0.5244 - val_accuracy: 0.7415
Epoch 3/20
220/220 [==============================] - 7s 30ms/step - loss: 0.2891 - accuracy: 0.8814 - val_loss: 0.5969 - val_accuracy: 0.7437
Epoch 4/20
220/220 [==============================] - 7s 30ms/step - loss: 0.2220 - accuracy: 0.9139 - val_loss: 0.6794 - val_accuracy: 0.7409
Epoch 5/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1696 - accuracy: 0.9336 - val_loss: 0.8170 - val_accuracy: 0.7443
Epoch 6/20
220/220 [==============================] - 7s 31ms/step - loss: 0.1316 - accuracy: 0.9513 - val_loss: 0.9054 - val_accuracy: 0.7227
Epoch 7/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1032 - accuracy: 0.9609 - val_loss: 1.0772 - val_accuracy: 0.7375
Epoch 8/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0888 - accuracy: 0.9677 - val_loss: 1.1497 - val_accuracy: 0.7403
Epoch 9/20
220/220 [==============================] - 7s 30ms/step - loss: 0.4548 - accuracy: 0.8575 - val_loss: 0.7483 - val_accuracy: 0.7193
Epoch 10/20
220/220 [==============================] - 7s 31ms/step - loss: 0.1875 - accuracy: 0.9273 - val_loss: 0.8865 - val_accuracy: 0.7205
Epoch 11/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1169 - accuracy: 0.9551 - val_loss: 1.0674 - val_accuracy: 0.7324
Epoch 12/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0874 - accuracy: 0.9663 - val_loss: 1.1714 - val_accuracy: 0.7341
Epoch 13/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1799 - accuracy: 0.9494 - val_loss: 1.6373 - val_accuracy: 0.6085
Epoch 14/20
220/220 [==============================] - 8s 35ms/step - loss: 0.2945 - accuracy: 0.8768 - val_loss: 0.8568 - val_accuracy: 0.7051
Epoch 15/20
220/220 [==============================] - 7s 34ms/step - loss: 0.2638 - accuracy: 0.8873 - val_loss: 0.9201 - val_accuracy: 0.7068
Epoch 16/20
220/220 [==============================] - 8s 36ms/step - loss: 0.1953 - accuracy: 0.9227 - val_loss: 0.9982 - val_accuracy: 0.7085
Epoch 17/20
220/220 [==============================] - 10s 44ms/step - loss: 0.1678 - accuracy: 0.9368 - val_loss: 1.0708 - val_accuracy: 0.7142
Epoch 18/20
220/220 [==============================] - 12s 56ms/step - loss: 0.1426 - accuracy: 0.9479 - val_loss: 1.1450 - val_accuracy: 0.7148
Epoch 19/20
220/220 [==============================] - 8s 35ms/step - loss: 0.1214 - accuracy: 0.9555 - val_loss: 1.2099 - val_accuracy: 0.7136
Epoch 20/20
220/220 [==============================] - 11s 49ms/step - loss: 0.0954 - accuracy: 0.9658 - val_loss: 1.2711 - val_accuracy: 0.7131
69/69 [==============================] - 1s 14ms/step - loss: 1.1314 - accuracy: 0.7395
Test Loss: 1.1314328908920288
Test Accuracy: 0.739545464515686
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_6&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_6 (Embedding)     (None, 50, 50)            1360900   
                                                                 
 bidirectional_6 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_6 (Dropout)         (None, 128)               0         
                                                                 
 dense_6 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 18ms/step - loss: 0.5952 - accuracy: 0.6674 - val_loss: 0.5262 - val_accuracy: 0.7239
Epoch 2/20
220/220 [==============================] - 3s 14ms/step - loss: 0.3909 - accuracy: 0.8242 - val_loss: 0.5358 - val_accuracy: 0.7489
Epoch 3/20
220/220 [==============================] - 3s 14ms/step - loss: 0.2818 - accuracy: 0.8868 - val_loss: 0.5852 - val_accuracy: 0.7500
Epoch 4/20
220/220 [==============================] - 3s 14ms/step - loss: 0.2156 - accuracy: 0.9146 - val_loss: 0.7183 - val_accuracy: 0.7455
Epoch 5/20
220/220 [==============================] - 3s 14ms/step - loss: 0.1659 - accuracy: 0.9356 - val_loss: 0.8273 - val_accuracy: 0.7489
Epoch 6/20
220/220 [==============================] - 3s 14ms/step - loss: 0.1294 - accuracy: 0.9538 - val_loss: 0.9561 - val_accuracy: 0.7347
Epoch 7/20
220/220 [==============================] - 3s 14ms/step - loss: 0.1008 - accuracy: 0.9606 - val_loss: 1.0557 - val_accuracy: 0.7409
Epoch 8/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0892 - accuracy: 0.9679 - val_loss: 1.0755 - val_accuracy: 0.7420
Epoch 9/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0695 - accuracy: 0.9746 - val_loss: 1.2737 - val_accuracy: 0.7398
Epoch 10/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0608 - accuracy: 0.9783 - val_loss: 1.4011 - val_accuracy: 0.7312
Epoch 11/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0570 - accuracy: 0.9794 - val_loss: 1.5462 - val_accuracy: 0.7358
Epoch 12/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0619 - accuracy: 0.9758 - val_loss: 1.3758 - val_accuracy: 0.7307
Epoch 13/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0523 - accuracy: 0.9804 - val_loss: 1.4624 - val_accuracy: 0.7352
Epoch 14/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0448 - accuracy: 0.9831 - val_loss: 1.5813 - val_accuracy: 0.7278
Epoch 15/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0433 - accuracy: 0.9838 - val_loss: 1.6742 - val_accuracy: 0.7250
Epoch 16/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0396 - accuracy: 0.9852 - val_loss: 1.6701 - val_accuracy: 0.7267
Epoch 17/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0397 - accuracy: 0.9848 - val_loss: 1.7883 - val_accuracy: 0.7261
Epoch 18/20
220/220 [==============================] - 3s 13ms/step - loss: 0.0360 - accuracy: 0.9855 - val_loss: 1.7947 - val_accuracy: 0.7256
Epoch 19/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0357 - accuracy: 0.9856 - val_loss: 1.9651 - val_accuracy: 0.7216
Epoch 20/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0436 - accuracy: 0.9831 - val_loss: 1.6644 - val_accuracy: 0.7188
69/69 [==============================] - 1s 8ms/step - loss: 1.4299 - accuracy: 0.7455
Test Loss: 1.4298816919326782
Test Accuracy: 0.7454545497894287
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_7&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_7 (Embedding)     (None, 100, 50)           1360900   
                                                                 
 bidirectional_7 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_7 (Dropout)         (None, 128)               0         
                                                                 
 dense_7 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 7s 23ms/step - loss: 0.6315 - accuracy: 0.6537 - val_loss: 0.5318 - val_accuracy: 0.7239
Epoch 2/20
220/220 [==============================] - 4s 19ms/step - loss: 0.3992 - accuracy: 0.8240 - val_loss: 0.5155 - val_accuracy: 0.7432
Epoch 3/20
220/220 [==============================] - 4s 19ms/step - loss: 0.2947 - accuracy: 0.8765 - val_loss: 0.6019 - val_accuracy: 0.7563
Epoch 4/20
220/220 [==============================] - 4s 19ms/step - loss: 0.2290 - accuracy: 0.9096 - val_loss: 0.6623 - val_accuracy: 0.7437
Epoch 5/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1936 - accuracy: 0.9270 - val_loss: 0.7365 - val_accuracy: 0.7074
Epoch 6/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1659 - accuracy: 0.9348 - val_loss: 0.8733 - val_accuracy: 0.7386
Epoch 7/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1329 - accuracy: 0.9494 - val_loss: 0.9693 - val_accuracy: 0.7375
Epoch 8/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1145 - accuracy: 0.9567 - val_loss: 1.1042 - val_accuracy: 0.7267
Epoch 9/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0973 - accuracy: 0.9618 - val_loss: 1.1955 - val_accuracy: 0.7386
Epoch 10/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0868 - accuracy: 0.9676 - val_loss: 1.2900 - val_accuracy: 0.7420
Epoch 11/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0730 - accuracy: 0.9731 - val_loss: 1.4293 - val_accuracy: 0.7392
Epoch 12/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0655 - accuracy: 0.9754 - val_loss: 1.4993 - val_accuracy: 0.7358
Epoch 13/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0589 - accuracy: 0.9768 - val_loss: 1.5890 - val_accuracy: 0.7352
Epoch 14/20
220/220 [==============================] - 5s 21ms/step - loss: 0.0548 - accuracy: 0.9788 - val_loss: 1.7023 - val_accuracy: 0.7358
Epoch 15/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0536 - accuracy: 0.9795 - val_loss: 1.6677 - val_accuracy: 0.7284
Epoch 16/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0477 - accuracy: 0.9814 - val_loss: 1.7524 - val_accuracy: 0.7312
Epoch 17/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0494 - accuracy: 0.9801 - val_loss: 1.8220 - val_accuracy: 0.7267
Epoch 18/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0449 - accuracy: 0.9822 - val_loss: 1.8583 - val_accuracy: 0.7290
Epoch 19/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0381 - accuracy: 0.9845 - val_loss: 1.9620 - val_accuracy: 0.7227
Epoch 20/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0358 - accuracy: 0.9842 - val_loss: 2.0493 - val_accuracy: 0.7278
69/69 [==============================] - 1s 8ms/step - loss: 1.9009 - accuracy: 0.7382
Test Loss: 1.9009497165679932
Test Accuracy: 0.7381818294525146
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_8&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_8 (Embedding)     (None, 200, 50)           1360900   
                                                                 
 bidirectional_8 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_8 (Dropout)         (None, 128)               0         
                                                                 
 dense_8 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 10s 35ms/step - loss: 0.6008 - accuracy: 0.6650 - val_loss: 0.5104 - val_accuracy: 0.7364
Epoch 2/20
220/220 [==============================] - 7s 31ms/step - loss: 0.3801 - accuracy: 0.8319 - val_loss: 0.5183 - val_accuracy: 0.7511
Epoch 3/20
220/220 [==============================] - 7s 31ms/step - loss: 0.2775 - accuracy: 0.8836 - val_loss: 0.5833 - val_accuracy: 0.7494
Epoch 4/20
220/220 [==============================] - 7s 30ms/step - loss: 0.2252 - accuracy: 0.9113 - val_loss: 0.7120 - val_accuracy: 0.7437
Epoch 5/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1718 - accuracy: 0.9311 - val_loss: 0.7853 - val_accuracy: 0.7426
Epoch 6/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1314 - accuracy: 0.9531 - val_loss: 0.9426 - val_accuracy: 0.7437
Epoch 7/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1078 - accuracy: 0.9588 - val_loss: 1.0727 - val_accuracy: 0.7392
Epoch 8/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0933 - accuracy: 0.9652 - val_loss: 1.1280 - val_accuracy: 0.7443
Epoch 9/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0801 - accuracy: 0.9683 - val_loss: 1.2436 - val_accuracy: 0.7375
Epoch 10/20
220/220 [==============================] - 6s 29ms/step - loss: 0.0737 - accuracy: 0.9730 - val_loss: 1.2487 - val_accuracy: 0.7403
Epoch 11/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0729 - accuracy: 0.9736 - val_loss: 1.3670 - val_accuracy: 0.7426
Epoch 12/20
220/220 [==============================] - 7s 32ms/step - loss: 0.0631 - accuracy: 0.9749 - val_loss: 1.3714 - val_accuracy: 0.7347
Epoch 13/20
220/220 [==============================] - 7s 31ms/step - loss: 0.0507 - accuracy: 0.9812 - val_loss: 1.5900 - val_accuracy: 0.7341
Epoch 14/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0446 - accuracy: 0.9820 - val_loss: 1.7889 - val_accuracy: 0.7392
Epoch 15/20
220/220 [==============================] - 7s 31ms/step - loss: 0.0440 - accuracy: 0.9824 - val_loss: 1.7168 - val_accuracy: 0.7415
Epoch 16/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0433 - accuracy: 0.9827 - val_loss: 1.8890 - val_accuracy: 0.7369
Epoch 17/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0391 - accuracy: 0.9847 - val_loss: 1.8630 - val_accuracy: 0.7364
Epoch 18/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 1.7702 - val_accuracy: 0.7375
Epoch 19/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0589 - accuracy: 0.9768 - val_loss: 1.4896 - val_accuracy: 0.7318
Epoch 20/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0485 - accuracy: 0.9812 - val_loss: 1.6971 - val_accuracy: 0.7324
69/69 [==============================] - 1s 14ms/step - loss: 1.5201 - accuracy: 0.7505
Test Loss: 1.5201168060302734
Test Accuracy: 0.7504545450210571
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_9&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_9 (Embedding)     (None, 50, 50)            1360900   
                                                                 
 bidirectional_9 (Bidirectio  (None, 128)              58880     
 nal)                                                            
                                                                 
 dropout_9 (Dropout)         (None, 128)               0         
                                                                 
 dense_9 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 17ms/step - loss: 0.6083 - accuracy: 0.6520 - val_loss: 0.5334 - val_accuracy: 0.7318
Epoch 2/20
220/220 [==============================] - 3s 14ms/step - loss: 0.3883 - accuracy: 0.8279 - val_loss: 0.5163 - val_accuracy: 0.7545
Epoch 3/20
220/220 [==============================] - 3s 14ms/step - loss: 0.2842 - accuracy: 0.8841 - val_loss: 0.5806 - val_accuracy: 0.7563
Epoch 4/20
220/220 [==============================] - 3s 14ms/step - loss: 0.2118 - accuracy: 0.9139 - val_loss: 0.6963 - val_accuracy: 0.7523
Epoch 5/20
220/220 [==============================] - 3s 14ms/step - loss: 0.1660 - accuracy: 0.9341 - val_loss: 0.7957 - val_accuracy: 0.7437
Epoch 6/20
220/220 [==============================] - 3s 14ms/step - loss: 0.1231 - accuracy: 0.9552 - val_loss: 0.9546 - val_accuracy: 0.7426
Epoch 7/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0994 - accuracy: 0.9635 - val_loss: 1.0590 - val_accuracy: 0.7375
Epoch 8/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0822 - accuracy: 0.9696 - val_loss: 1.1815 - val_accuracy: 0.7392
Epoch 9/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 1.1800 - val_accuracy: 0.7341
Epoch 10/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0633 - accuracy: 0.9773 - val_loss: 1.3599 - val_accuracy: 0.7364
Epoch 11/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0565 - accuracy: 0.9785 - val_loss: 1.3473 - val_accuracy: 0.7358
Epoch 12/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0524 - accuracy: 0.9817 - val_loss: 1.4989 - val_accuracy: 0.7341
Epoch 13/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0476 - accuracy: 0.9818 - val_loss: 1.6267 - val_accuracy: 0.7341
Epoch 14/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0449 - accuracy: 0.9828 - val_loss: 1.6054 - val_accuracy: 0.7386
Epoch 15/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0496 - accuracy: 0.9828 - val_loss: 1.5607 - val_accuracy: 0.7364
Epoch 16/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0479 - accuracy: 0.9814 - val_loss: 1.5172 - val_accuracy: 0.7312
Epoch 17/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0399 - accuracy: 0.9842 - val_loss: 1.6698 - val_accuracy: 0.7284
Epoch 18/20
220/220 [==============================] - 3s 13ms/step - loss: 0.0390 - accuracy: 0.9845 - val_loss: 1.7729 - val_accuracy: 0.7341
Epoch 19/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0381 - accuracy: 0.9842 - val_loss: 1.8497 - val_accuracy: 0.7341
Epoch 20/20
220/220 [==============================] - 3s 14ms/step - loss: 0.0403 - accuracy: 0.9842 - val_loss: 1.8345 - val_accuracy: 0.7278
69/69 [==============================] - 1s 6ms/step - loss: 1.6254 - accuracy: 0.7482
Test Loss: 1.6253750324249268
Test Accuracy: 0.7481818199157715
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_10&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_10 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_10 (Bidirecti  (None, 128)              58880     
 onal)                                                           
                                                                 
 dropout_10 (Dropout)        (None, 128)               0         
                                                                 
 dense_10 (Dense)            (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 7s 25ms/step - loss: 0.6037 - accuracy: 0.6594 - val_loss: 0.5331 - val_accuracy: 0.7369
Epoch 2/20
220/220 [==============================] - 4s 19ms/step - loss: 0.3886 - accuracy: 0.8323 - val_loss: 0.5278 - val_accuracy: 0.7540
Epoch 3/20
220/220 [==============================] - 4s 19ms/step - loss: 0.2904 - accuracy: 0.8829 - val_loss: 0.5946 - val_accuracy: 0.7489
Epoch 4/20
220/220 [==============================] - 4s 19ms/step - loss: 0.2222 - accuracy: 0.9119 - val_loss: 0.6790 - val_accuracy: 0.7534
Epoch 5/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1739 - accuracy: 0.9349 - val_loss: 0.7707 - val_accuracy: 0.7375
Epoch 6/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1382 - accuracy: 0.9473 - val_loss: 0.8598 - val_accuracy: 0.7409
Epoch 7/20
220/220 [==============================] - 4s 19ms/step - loss: 0.1161 - accuracy: 0.9571 - val_loss: 0.9703 - val_accuracy: 0.7375
Epoch 8/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0936 - accuracy: 0.9656 - val_loss: 1.0696 - val_accuracy: 0.7426
Epoch 9/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0826 - accuracy: 0.9702 - val_loss: 1.2256 - val_accuracy: 0.7358
Epoch 10/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0698 - accuracy: 0.9756 - val_loss: 1.2521 - val_accuracy: 0.7403
Epoch 11/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0583 - accuracy: 0.9790 - val_loss: 1.4117 - val_accuracy: 0.7403
Epoch 12/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0503 - accuracy: 0.9807 - val_loss: 1.5010 - val_accuracy: 0.7392
Epoch 13/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0514 - accuracy: 0.9798 - val_loss: 1.4432 - val_accuracy: 0.7364
Epoch 14/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0507 - accuracy: 0.9814 - val_loss: 1.5317 - val_accuracy: 0.7381
Epoch 15/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0447 - accuracy: 0.9825 - val_loss: 1.7091 - val_accuracy: 0.7176
Epoch 16/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0438 - accuracy: 0.9838 - val_loss: 1.5954 - val_accuracy: 0.7324
Epoch 17/20
220/220 [==============================] - 4s 20ms/step - loss: 0.0522 - accuracy: 0.9805 - val_loss: 1.5970 - val_accuracy: 0.7324
Epoch 18/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0416 - accuracy: 0.9834 - val_loss: 1.6356 - val_accuracy: 0.7290
Epoch 19/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0374 - accuracy: 0.9854 - val_loss: 1.7913 - val_accuracy: 0.7278
Epoch 20/20
220/220 [==============================] - 4s 19ms/step - loss: 0.0347 - accuracy: 0.9851 - val_loss: 2.0019 - val_accuracy: 0.7318
69/69 [==============================] - 1s 8ms/step - loss: 1.8259 - accuracy: 0.7482
Test Loss: 1.8258705139160156
Test Accuracy: 0.7481818199157715
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 64, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_11&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_11 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_11 (Bidirecti  (None, 128)              58880     
 onal)                                                           
                                                                 
 dropout_11 (Dropout)        (None, 128)               0         
                                                                 
 dense_11 (Dense)            (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,419,909
Trainable params: 1,419,909
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 9s 33ms/step - loss: 0.5952 - accuracy: 0.6726 - val_loss: 0.5120 - val_accuracy: 0.7420
Epoch 2/20
220/220 [==============================] - 7s 30ms/step - loss: 0.3890 - accuracy: 0.8265 - val_loss: 0.5310 - val_accuracy: 0.7517
Epoch 3/20
220/220 [==============================] - 7s 30ms/step - loss: 0.3091 - accuracy: 0.8747 - val_loss: 0.5718 - val_accuracy: 0.7574
Epoch 4/20
220/220 [==============================] - 7s 30ms/step - loss: 0.2330 - accuracy: 0.9071 - val_loss: 0.6553 - val_accuracy: 0.7506
Epoch 5/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1867 - accuracy: 0.9290 - val_loss: 0.7631 - val_accuracy: 0.7517
Epoch 6/20
220/220 [==============================] - 7s 31ms/step - loss: 0.1595 - accuracy: 0.9413 - val_loss: 0.8693 - val_accuracy: 0.7415
Epoch 7/20
220/220 [==============================] - 7s 31ms/step - loss: 0.1350 - accuracy: 0.9494 - val_loss: 0.9901 - val_accuracy: 0.7460
Epoch 8/20
220/220 [==============================] - 7s 30ms/step - loss: 0.1101 - accuracy: 0.9614 - val_loss: 1.0862 - val_accuracy: 0.7432
Epoch 9/20
220/220 [==============================] - 7s 33ms/step - loss: 0.0947 - accuracy: 0.9653 - val_loss: 1.1149 - val_accuracy: 0.7437
Epoch 10/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0876 - accuracy: 0.9676 - val_loss: 1.1306 - val_accuracy: 0.7364
Epoch 11/20
220/220 [==============================] - 7s 31ms/step - loss: 0.0713 - accuracy: 0.9749 - val_loss: 1.3880 - val_accuracy: 0.7409
Epoch 12/20
220/220 [==============================] - 6s 29ms/step - loss: 0.0604 - accuracy: 0.9794 - val_loss: 1.4740 - val_accuracy: 0.7358
Epoch 13/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0544 - accuracy: 0.9798 - val_loss: 1.5566 - val_accuracy: 0.7364
Epoch 14/20
220/220 [==============================] - 7s 31ms/step - loss: 0.0521 - accuracy: 0.9811 - val_loss: 1.6394 - val_accuracy: 0.7227
Epoch 15/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0541 - accuracy: 0.9798 - val_loss: 1.6641 - val_accuracy: 0.7301
Epoch 16/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0492 - accuracy: 0.9825 - val_loss: 1.6487 - val_accuracy: 0.7256
Epoch 17/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0452 - accuracy: 0.9824 - val_loss: 1.7856 - val_accuracy: 0.7295
Epoch 18/20
220/220 [==============================] - 7s 31ms/step - loss: 0.0405 - accuracy: 0.9845 - val_loss: 1.7837 - val_accuracy: 0.7301
Epoch 19/20
220/220 [==============================] - 7s 31ms/step - loss: 0.0393 - accuracy: 0.9851 - val_loss: 1.8383 - val_accuracy: 0.7324
Epoch 20/20
220/220 [==============================] - 7s 30ms/step - loss: 0.0379 - accuracy: 0.9837 - val_loss: 1.9164 - val_accuracy: 0.7284
69/69 [==============================] - 1s 13ms/step - loss: 1.7814 - accuracy: 0.7373
Test Loss: 1.7814329862594604
Test Accuracy: 0.7372727394104004
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_12&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_12 (Embedding)    (None, 50, 50)            1360900   
                                                                 
 bidirectional_12 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_12 (Dropout)        (None, 256)               0         
                                                                 
 dense_12 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 19ms/step - loss: 0.5995 - accuracy: 0.6628 - val_loss: 0.5171 - val_accuracy: 0.7455
Epoch 2/20
220/220 [==============================] - 3s 16ms/step - loss: 0.3839 - accuracy: 0.8305 - val_loss: 0.5251 - val_accuracy: 0.7506
Epoch 3/20
220/220 [==============================] - 4s 16ms/step - loss: 0.2785 - accuracy: 0.8831 - val_loss: 0.6185 - val_accuracy: 0.7517
Epoch 4/20
220/220 [==============================] - 4s 16ms/step - loss: 0.2151 - accuracy: 0.9139 - val_loss: 0.7002 - val_accuracy: 0.7483
Epoch 5/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1691 - accuracy: 0.9338 - val_loss: 0.7983 - val_accuracy: 0.7403
Epoch 6/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1408 - accuracy: 0.9473 - val_loss: 0.8441 - val_accuracy: 0.7381
Epoch 7/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1129 - accuracy: 0.9568 - val_loss: 1.1024 - val_accuracy: 0.7392
Epoch 8/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0859 - accuracy: 0.9677 - val_loss: 1.1710 - val_accuracy: 0.7437
Epoch 9/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0734 - accuracy: 0.9734 - val_loss: 1.2759 - val_accuracy: 0.7392
Epoch 10/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0628 - accuracy: 0.9761 - val_loss: 1.4360 - val_accuracy: 0.7335
Epoch 11/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0589 - accuracy: 0.9783 - val_loss: 1.5116 - val_accuracy: 0.7250
Epoch 12/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0552 - accuracy: 0.9791 - val_loss: 1.5498 - val_accuracy: 0.7278
Epoch 13/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0478 - accuracy: 0.9828 - val_loss: 1.5869 - val_accuracy: 0.7335
Epoch 14/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0459 - accuracy: 0.9821 - val_loss: 1.7806 - val_accuracy: 0.7261
Epoch 15/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0504 - accuracy: 0.9815 - val_loss: 1.6878 - val_accuracy: 0.7290
Epoch 16/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0435 - accuracy: 0.9834 - val_loss: 1.6937 - val_accuracy: 0.7307
Epoch 17/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0432 - accuracy: 0.9822 - val_loss: 1.7448 - val_accuracy: 0.7182
Epoch 18/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0410 - accuracy: 0.9841 - val_loss: 1.7334 - val_accuracy: 0.7273
Epoch 19/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0370 - accuracy: 0.9852 - val_loss: 1.8607 - val_accuracy: 0.7227
Epoch 20/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0344 - accuracy: 0.9856 - val_loss: 2.0618 - val_accuracy: 0.7278
69/69 [==============================] - 1s 7ms/step - loss: 1.8794 - accuracy: 0.7386
Test Loss: 1.8794431686401367
Test Accuracy: 0.7386363744735718
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_13&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_13 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_13 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_13 (Dropout)        (None, 256)               0         
                                                                 
 dense_13 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 9s 28ms/step - loss: 0.5907 - accuracy: 0.6672 - val_loss: 0.5127 - val_accuracy: 0.7437
Epoch 2/20
220/220 [==============================] - 5s 24ms/step - loss: 0.3816 - accuracy: 0.8311 - val_loss: 0.5223 - val_accuracy: 0.7392
Epoch 3/20
220/220 [==============================] - 5s 24ms/step - loss: 0.2863 - accuracy: 0.8836 - val_loss: 0.5837 - val_accuracy: 0.7523
Epoch 4/20
220/220 [==============================] - 5s 24ms/step - loss: 0.2223 - accuracy: 0.9106 - val_loss: 0.6679 - val_accuracy: 0.7494
Epoch 5/20
220/220 [==============================] - 6s 26ms/step - loss: 0.1770 - accuracy: 0.9352 - val_loss: 0.7998 - val_accuracy: 0.7409
Epoch 6/20
220/220 [==============================] - 5s 24ms/step - loss: 0.1376 - accuracy: 0.9479 - val_loss: 0.9407 - val_accuracy: 0.7443
Epoch 7/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1024 - accuracy: 0.9611 - val_loss: 1.0151 - val_accuracy: 0.7312
Epoch 8/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0832 - accuracy: 0.9685 - val_loss: 1.1888 - val_accuracy: 0.7369
Epoch 9/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0724 - accuracy: 0.9737 - val_loss: 1.2550 - val_accuracy: 0.7352
Epoch 10/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0620 - accuracy: 0.9770 - val_loss: 1.4413 - val_accuracy: 0.7324
Epoch 11/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0525 - accuracy: 0.9805 - val_loss: 1.5076 - val_accuracy: 0.7239
Epoch 12/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 1.6621 - val_accuracy: 0.7261
Epoch 13/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0481 - accuracy: 0.9820 - val_loss: 1.6262 - val_accuracy: 0.7295
Epoch 14/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0438 - accuracy: 0.9841 - val_loss: 1.7541 - val_accuracy: 0.7364
Epoch 15/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0454 - accuracy: 0.9831 - val_loss: 1.7104 - val_accuracy: 0.7290
Epoch 16/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0425 - accuracy: 0.9828 - val_loss: 1.6382 - val_accuracy: 0.7295
Epoch 17/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0412 - accuracy: 0.9851 - val_loss: 1.6883 - val_accuracy: 0.7330
Epoch 18/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0382 - accuracy: 0.9852 - val_loss: 1.8956 - val_accuracy: 0.7307
Epoch 19/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0395 - accuracy: 0.9832 - val_loss: 1.8835 - val_accuracy: 0.7307
Epoch 20/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0385 - accuracy: 0.9842 - val_loss: 2.0642 - val_accuracy: 0.7261
69/69 [==============================] - 1s 9ms/step - loss: 1.8504 - accuracy: 0.7323
Test Loss: 1.8504351377487183
Test Accuracy: 0.732272744178772
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_14&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_14 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_14 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_14 (Dropout)        (None, 256)               0         
                                                                 
 dense_14 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 11s 43ms/step - loss: 0.5940 - accuracy: 0.6721 - val_loss: 0.5156 - val_accuracy: 0.7403
Epoch 2/20
220/220 [==============================] - 9s 41ms/step - loss: 0.4392 - accuracy: 0.7984 - val_loss: 0.6085 - val_accuracy: 0.6750
Epoch 3/20
220/220 [==============================] - 9s 40ms/step - loss: 0.3942 - accuracy: 0.8281 - val_loss: 0.5783 - val_accuracy: 0.7460
Epoch 4/20
220/220 [==============================] - 9s 40ms/step - loss: 0.2662 - accuracy: 0.8879 - val_loss: 0.6329 - val_accuracy: 0.7477
Epoch 5/20
220/220 [==============================] - 9s 40ms/step - loss: 0.2148 - accuracy: 0.9123 - val_loss: 0.7217 - val_accuracy: 0.7403
Epoch 6/20
220/220 [==============================] - 9s 40ms/step - loss: 0.1827 - accuracy: 0.9265 - val_loss: 0.8646 - val_accuracy: 0.7506
Epoch 7/20
220/220 [==============================] - 9s 40ms/step - loss: 0.1538 - accuracy: 0.9402 - val_loss: 0.9210 - val_accuracy: 0.7455
Epoch 8/20
220/220 [==============================] - 9s 42ms/step - loss: 0.1422 - accuracy: 0.9453 - val_loss: 1.0640 - val_accuracy: 0.7403
Epoch 9/20
220/220 [==============================] - 9s 40ms/step - loss: 0.1215 - accuracy: 0.9501 - val_loss: 1.0093 - val_accuracy: 0.7398
Epoch 10/20
220/220 [==============================] - 9s 40ms/step - loss: 0.1118 - accuracy: 0.9571 - val_loss: 1.1499 - val_accuracy: 0.7398
Epoch 11/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0983 - accuracy: 0.9638 - val_loss: 1.3767 - val_accuracy: 0.7324
Epoch 12/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0853 - accuracy: 0.9650 - val_loss: 1.4936 - val_accuracy: 0.7330
Epoch 13/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0799 - accuracy: 0.9682 - val_loss: 1.4159 - val_accuracy: 0.7330
Epoch 14/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0815 - accuracy: 0.9687 - val_loss: 1.5291 - val_accuracy: 0.7256
Epoch 15/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0744 - accuracy: 0.9727 - val_loss: 1.5969 - val_accuracy: 0.7261
Epoch 16/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0698 - accuracy: 0.9723 - val_loss: 1.7743 - val_accuracy: 0.7216
Epoch 17/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0606 - accuracy: 0.9766 - val_loss: 1.8994 - val_accuracy: 0.7284
Epoch 18/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0542 - accuracy: 0.9790 - val_loss: 2.0531 - val_accuracy: 0.7199
Epoch 19/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0504 - accuracy: 0.9811 - val_loss: 2.1615 - val_accuracy: 0.7244
Epoch 20/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0457 - accuracy: 0.9812 - val_loss: 2.2301 - val_accuracy: 0.7210
69/69 [==============================] - 2s 15ms/step - loss: 2.0587 - accuracy: 0.7386
Test Loss: 2.058682441711426
Test Accuracy: 0.7386363744735718
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_15&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_15 (Embedding)    (None, 50, 50)            1360900   
                                                                 
 bidirectional_15 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_15 (Dropout)        (None, 256)               0         
                                                                 
 dense_15 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 20ms/step - loss: 0.6033 - accuracy: 0.6647 - val_loss: 0.5285 - val_accuracy: 0.7205
Epoch 2/20
220/220 [==============================] - 4s 17ms/step - loss: 0.3847 - accuracy: 0.8242 - val_loss: 0.5186 - val_accuracy: 0.7580
Epoch 3/20
220/220 [==============================] - 4s 17ms/step - loss: 0.2908 - accuracy: 0.8819 - val_loss: 0.6130 - val_accuracy: 0.7557
Epoch 4/20
220/220 [==============================] - 4s 17ms/step - loss: 0.2191 - accuracy: 0.9111 - val_loss: 0.6688 - val_accuracy: 0.7511
Epoch 5/20
220/220 [==============================] - 4s 17ms/step - loss: 0.1636 - accuracy: 0.9399 - val_loss: 0.7967 - val_accuracy: 0.7443
Epoch 6/20
220/220 [==============================] - 4s 17ms/step - loss: 0.1292 - accuracy: 0.9523 - val_loss: 0.9248 - val_accuracy: 0.7409
Epoch 7/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0984 - accuracy: 0.9648 - val_loss: 1.0035 - val_accuracy: 0.7455
Epoch 8/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0772 - accuracy: 0.9714 - val_loss: 1.2136 - val_accuracy: 0.7381
Epoch 9/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0657 - accuracy: 0.9753 - val_loss: 1.2951 - val_accuracy: 0.7369
Epoch 10/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0591 - accuracy: 0.9783 - val_loss: 1.4258 - val_accuracy: 0.7284
Epoch 11/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0533 - accuracy: 0.9804 - val_loss: 1.4630 - val_accuracy: 0.7244
Epoch 12/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0495 - accuracy: 0.9817 - val_loss: 1.4606 - val_accuracy: 0.7347
Epoch 13/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0468 - accuracy: 0.9834 - val_loss: 1.5776 - val_accuracy: 0.7278
Epoch 14/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0430 - accuracy: 0.9831 - val_loss: 1.5909 - val_accuracy: 0.7273
Epoch 15/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0420 - accuracy: 0.9841 - val_loss: 1.7005 - val_accuracy: 0.7335
Epoch 16/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0368 - accuracy: 0.9851 - val_loss: 1.8113 - val_accuracy: 0.7324
Epoch 17/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0361 - accuracy: 0.9859 - val_loss: 1.8301 - val_accuracy: 0.7216
Epoch 18/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0351 - accuracy: 0.9852 - val_loss: 2.0307 - val_accuracy: 0.7295
Epoch 19/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0347 - accuracy: 0.9851 - val_loss: 2.1937 - val_accuracy: 0.7364
Epoch 20/20
220/220 [==============================] - 5s 21ms/step - loss: 0.0468 - accuracy: 0.9817 - val_loss: 1.6993 - val_accuracy: 0.7222
69/69 [==============================] - 1s 7ms/step - loss: 1.5324 - accuracy: 0.7414
Test Loss: 1.5323894023895264
Test Accuracy: 0.7413636445999146
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_16&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_16 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_16 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_16 (Dropout)        (None, 256)               0         
                                                                 
 dense_16 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 8s 28ms/step - loss: 0.6075 - accuracy: 0.6688 - val_loss: 0.5304 - val_accuracy: 0.7199
Epoch 2/20
220/220 [==============================] - 6s 25ms/step - loss: 0.3864 - accuracy: 0.8281 - val_loss: 0.5278 - val_accuracy: 0.7500
Epoch 3/20
220/220 [==============================] - 6s 25ms/step - loss: 0.2829 - accuracy: 0.8846 - val_loss: 0.5974 - val_accuracy: 0.7256
Epoch 4/20
220/220 [==============================] - 5s 25ms/step - loss: 0.2719 - accuracy: 0.8868 - val_loss: 0.6757 - val_accuracy: 0.7386
Epoch 5/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1931 - accuracy: 0.9236 - val_loss: 0.7683 - val_accuracy: 0.7443
Epoch 6/20
220/220 [==============================] - 5s 24ms/step - loss: 0.1435 - accuracy: 0.9469 - val_loss: 0.8642 - val_accuracy: 0.7409
Epoch 7/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1123 - accuracy: 0.9571 - val_loss: 1.0089 - val_accuracy: 0.7398
Epoch 8/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0907 - accuracy: 0.9670 - val_loss: 1.1495 - val_accuracy: 0.7426
Epoch 9/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0747 - accuracy: 0.9734 - val_loss: 1.3055 - val_accuracy: 0.7386
Epoch 10/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0671 - accuracy: 0.9768 - val_loss: 1.3477 - val_accuracy: 0.7341
Epoch 11/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0597 - accuracy: 0.9784 - val_loss: 1.5224 - val_accuracy: 0.7392
Epoch 12/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0680 - accuracy: 0.9751 - val_loss: 1.3035 - val_accuracy: 0.7312
Epoch 13/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 1.4829 - val_accuracy: 0.7335
Epoch 14/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0448 - accuracy: 0.9838 - val_loss: 1.5844 - val_accuracy: 0.7324
Epoch 15/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0435 - accuracy: 0.9835 - val_loss: 1.7271 - val_accuracy: 0.7324
Epoch 16/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0400 - accuracy: 0.9841 - val_loss: 1.7895 - val_accuracy: 0.7324
Epoch 17/20
220/220 [==============================] - 8s 35ms/step - loss: 0.0380 - accuracy: 0.9854 - val_loss: 1.8953 - val_accuracy: 0.7341
Epoch 18/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0397 - accuracy: 0.9835 - val_loss: 1.8201 - val_accuracy: 0.7369
Epoch 19/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0399 - accuracy: 0.9832 - val_loss: 1.8430 - val_accuracy: 0.7284
Epoch 20/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0374 - accuracy: 0.9842 - val_loss: 1.9577 - val_accuracy: 0.7341
69/69 [==============================] - 1s 8ms/step - loss: 1.8474 - accuracy: 0.7477
Test Loss: 1.8474093675613403
Test Accuracy: 0.7477272748947144
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_17&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_17 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_17 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_17 (Dropout)        (None, 256)               0         
                                                                 
 dense_17 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 11s 41ms/step - loss: 0.7080 - accuracy: 0.6091 - val_loss: 0.6500 - val_accuracy: 0.6045
Epoch 2/20
220/220 [==============================] - 8s 38ms/step - loss: 0.4913 - accuracy: 0.7624 - val_loss: 0.5437 - val_accuracy: 0.7182
Epoch 3/20
220/220 [==============================] - 8s 39ms/step - loss: 0.3575 - accuracy: 0.8500 - val_loss: 0.5785 - val_accuracy: 0.7347
Epoch 4/20
220/220 [==============================] - 9s 40ms/step - loss: 0.2769 - accuracy: 0.8936 - val_loss: 0.6226 - val_accuracy: 0.7364
Epoch 5/20
220/220 [==============================] - 9s 40ms/step - loss: 0.2228 - accuracy: 0.9165 - val_loss: 0.7004 - val_accuracy: 0.7426
Epoch 6/20
220/220 [==============================] - 9s 39ms/step - loss: 0.1824 - accuracy: 0.9328 - val_loss: 0.7883 - val_accuracy: 0.7261
Epoch 7/20
220/220 [==============================] - 9s 39ms/step - loss: 0.1481 - accuracy: 0.9446 - val_loss: 0.8772 - val_accuracy: 0.7403
Epoch 8/20
220/220 [==============================] - 9s 39ms/step - loss: 0.1245 - accuracy: 0.9540 - val_loss: 0.9900 - val_accuracy: 0.7335
Epoch 9/20
220/220 [==============================] - 8s 38ms/step - loss: 0.1124 - accuracy: 0.9595 - val_loss: 1.0560 - val_accuracy: 0.7136
Epoch 10/20
220/220 [==============================] - 8s 38ms/step - loss: 0.0889 - accuracy: 0.9669 - val_loss: 1.1056 - val_accuracy: 0.7324
Epoch 11/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0794 - accuracy: 0.9713 - val_loss: 1.2448 - val_accuracy: 0.7205
Epoch 12/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0821 - accuracy: 0.9704 - val_loss: 1.2516 - val_accuracy: 0.7273
Epoch 13/20
220/220 [==============================] - 10s 45ms/step - loss: 0.0622 - accuracy: 0.9766 - val_loss: 1.3730 - val_accuracy: 0.7301
Epoch 14/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0590 - accuracy: 0.9778 - val_loss: 1.4540 - val_accuracy: 0.7210
Epoch 15/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0511 - accuracy: 0.9817 - val_loss: 1.5434 - val_accuracy: 0.7205
Epoch 16/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0484 - accuracy: 0.9822 - val_loss: 1.6216 - val_accuracy: 0.7182
Epoch 17/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0487 - accuracy: 0.9817 - val_loss: 1.6935 - val_accuracy: 0.7193
Epoch 18/20
220/220 [==============================] - 8s 38ms/step - loss: 0.0444 - accuracy: 0.9844 - val_loss: 1.7305 - val_accuracy: 0.7188
Epoch 19/20
220/220 [==============================] - 8s 38ms/step - loss: 0.0426 - accuracy: 0.9838 - val_loss: 1.8073 - val_accuracy: 0.7182
Epoch 20/20
220/220 [==============================] - 8s 38ms/step - loss: 0.0428 - accuracy: 0.9832 - val_loss: 1.7982 - val_accuracy: 0.7250
69/69 [==============================] - 1s 15ms/step - loss: 1.7058 - accuracy: 0.7468
Test Loss: 1.7058476209640503
Test Accuracy: 0.7468181848526001
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_18&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_18 (Embedding)    (None, 50, 50)            1360900   
                                                                 
 bidirectional_18 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_18 (Dropout)        (None, 256)               0         
                                                                 
 dense_18 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 18ms/step - loss: 0.5956 - accuracy: 0.6672 - val_loss: 0.5197 - val_accuracy: 0.7352
Epoch 2/20
220/220 [==============================] - 4s 17ms/step - loss: 0.3853 - accuracy: 0.8231 - val_loss: 0.5216 - val_accuracy: 0.7523
Epoch 3/20
220/220 [==============================] - 4s 16ms/step - loss: 0.2797 - accuracy: 0.8860 - val_loss: 0.5820 - val_accuracy: 0.7466
Epoch 4/20
220/220 [==============================] - 4s 17ms/step - loss: 0.2147 - accuracy: 0.9169 - val_loss: 0.6923 - val_accuracy: 0.7477
Epoch 5/20
220/220 [==============================] - 3s 16ms/step - loss: 0.1752 - accuracy: 0.9317 - val_loss: 0.8097 - val_accuracy: 0.7398
Epoch 6/20
220/220 [==============================] - 3s 16ms/step - loss: 0.1367 - accuracy: 0.9508 - val_loss: 0.9429 - val_accuracy: 0.7409
Epoch 7/20
220/220 [==============================] - 3s 16ms/step - loss: 0.1070 - accuracy: 0.9584 - val_loss: 1.0965 - val_accuracy: 0.7415
Epoch 8/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1184 - accuracy: 0.9550 - val_loss: 1.0783 - val_accuracy: 0.7227
Epoch 9/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0812 - accuracy: 0.9668 - val_loss: 1.2777 - val_accuracy: 0.7324
Epoch 10/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0671 - accuracy: 0.9760 - val_loss: 1.3662 - val_accuracy: 0.7267
Epoch 11/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0561 - accuracy: 0.9787 - val_loss: 1.4693 - val_accuracy: 0.7290
Epoch 12/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0505 - accuracy: 0.9822 - val_loss: 1.7226 - val_accuracy: 0.7324
Epoch 13/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0453 - accuracy: 0.9831 - val_loss: 1.7841 - val_accuracy: 0.7273
Epoch 14/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0450 - accuracy: 0.9829 - val_loss: 1.6763 - val_accuracy: 0.7250
Epoch 15/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0448 - accuracy: 0.9834 - val_loss: 1.6173 - val_accuracy: 0.7307
Epoch 16/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0414 - accuracy: 0.9838 - val_loss: 1.8669 - val_accuracy: 0.7330
Epoch 17/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0401 - accuracy: 0.9841 - val_loss: 1.8301 - val_accuracy: 0.7318
Epoch 18/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0367 - accuracy: 0.9851 - val_loss: 2.0413 - val_accuracy: 0.7307
Epoch 19/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0345 - accuracy: 0.9849 - val_loss: 2.1133 - val_accuracy: 0.7273
Epoch 20/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0335 - accuracy: 0.9859 - val_loss: 2.1980 - val_accuracy: 0.7347
69/69 [==============================] - 1s 7ms/step - loss: 1.9876 - accuracy: 0.7364
Test Loss: 1.9876080751419067
Test Accuracy: 0.7363636493682861
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_19&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_19 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_19 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_19 (Dropout)        (None, 256)               0         
                                                                 
 dense_19 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 7s 26ms/step - loss: 0.5866 - accuracy: 0.6743 - val_loss: 0.5063 - val_accuracy: 0.7511
Epoch 2/20
220/220 [==============================] - 5s 24ms/step - loss: 0.3748 - accuracy: 0.8333 - val_loss: 0.5281 - val_accuracy: 0.7557
Epoch 3/20
220/220 [==============================] - 5s 23ms/step - loss: 0.2731 - accuracy: 0.8873 - val_loss: 0.6313 - val_accuracy: 0.7568
Epoch 4/20
220/220 [==============================] - 5s 24ms/step - loss: 0.2184 - accuracy: 0.9129 - val_loss: 0.7222 - val_accuracy: 0.7455
Epoch 5/20
220/220 [==============================] - 5s 23ms/step - loss: 0.1749 - accuracy: 0.9346 - val_loss: 0.8638 - val_accuracy: 0.7432
Epoch 6/20
220/220 [==============================] - 5s 24ms/step - loss: 0.1463 - accuracy: 0.9464 - val_loss: 0.9230 - val_accuracy: 0.7443
Epoch 7/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1261 - accuracy: 0.9513 - val_loss: 1.0285 - val_accuracy: 0.7403
Epoch 8/20
220/220 [==============================] - 5s 24ms/step - loss: 0.1091 - accuracy: 0.9582 - val_loss: 1.1799 - val_accuracy: 0.7364
Epoch 9/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0894 - accuracy: 0.9658 - val_loss: 1.3144 - val_accuracy: 0.7386
Epoch 10/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0824 - accuracy: 0.9675 - val_loss: 1.4157 - val_accuracy: 0.7358
Epoch 11/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0677 - accuracy: 0.9749 - val_loss: 1.4552 - val_accuracy: 0.7341
Epoch 12/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0631 - accuracy: 0.9778 - val_loss: 1.7105 - val_accuracy: 0.7295
Epoch 13/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0534 - accuracy: 0.9800 - val_loss: 1.7110 - val_accuracy: 0.7295
Epoch 14/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0482 - accuracy: 0.9812 - val_loss: 1.7014 - val_accuracy: 0.7347
Epoch 15/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0444 - accuracy: 0.9824 - val_loss: 1.7793 - val_accuracy: 0.7341
Epoch 16/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0423 - accuracy: 0.9832 - val_loss: 1.9034 - val_accuracy: 0.7273
Epoch 17/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0415 - accuracy: 0.9837 - val_loss: 1.9604 - val_accuracy: 0.7244
Epoch 18/20
220/220 [==============================] - 5s 23ms/step - loss: 0.0416 - accuracy: 0.9831 - val_loss: 1.9119 - val_accuracy: 0.7261
Epoch 19/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0430 - accuracy: 0.9834 - val_loss: 1.8953 - val_accuracy: 0.7250
Epoch 20/20
220/220 [==============================] - 5s 22ms/step - loss: 0.0424 - accuracy: 0.9829 - val_loss: 1.8528 - val_accuracy: 0.7244
69/69 [==============================] - 1s 8ms/step - loss: 1.6752 - accuracy: 0.7359
Test Loss: 1.675197958946228
Test Accuracy: 0.735909104347229
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 128, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_20&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_20 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_20 (Bidirecti  (None, 256)              183296    
 onal)                                                           
                                                                 
 dropout_20 (Dropout)        (None, 256)               0         
                                                                 
 dense_20 (Dense)            (None, 1)                 257       
                                                                 
=================================================================
Total params: 1,544,453
Trainable params: 1,544,453
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 11s 41ms/step - loss: 0.5901 - accuracy: 0.6745 - val_loss: 0.5129 - val_accuracy: 0.7347
Epoch 2/20
220/220 [==============================] - 8s 39ms/step - loss: 0.3858 - accuracy: 0.8298 - val_loss: 0.5275 - val_accuracy: 0.7398
Epoch 3/20
220/220 [==============================] - 8s 38ms/step - loss: 0.2791 - accuracy: 0.8795 - val_loss: 0.5944 - val_accuracy: 0.7506
Epoch 4/20
220/220 [==============================] - 8s 38ms/step - loss: 0.2190 - accuracy: 0.9140 - val_loss: 0.7119 - val_accuracy: 0.7420
Epoch 5/20
220/220 [==============================] - 8s 38ms/step - loss: 0.1853 - accuracy: 0.9291 - val_loss: 0.7954 - val_accuracy: 0.7409
Epoch 6/20
220/220 [==============================] - 9s 40ms/step - loss: 0.1542 - accuracy: 0.9425 - val_loss: 0.8965 - val_accuracy: 0.7386
Epoch 7/20
220/220 [==============================] - 9s 41ms/step - loss: 0.1238 - accuracy: 0.9550 - val_loss: 0.9970 - val_accuracy: 0.7341
Epoch 8/20
220/220 [==============================] - 9s 39ms/step - loss: 0.1007 - accuracy: 0.9636 - val_loss: 1.1020 - val_accuracy: 0.7307
Epoch 9/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0897 - accuracy: 0.9665 - val_loss: 1.2448 - val_accuracy: 0.7364
Epoch 10/20
220/220 [==============================] - 8s 39ms/step - loss: 0.0737 - accuracy: 0.9743 - val_loss: 1.2246 - val_accuracy: 0.7318
Epoch 11/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0592 - accuracy: 0.9781 - val_loss: 1.5413 - val_accuracy: 0.7278
Epoch 12/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0544 - accuracy: 0.9818 - val_loss: 1.5683 - val_accuracy: 0.7301
Epoch 13/20
220/220 [==============================] - 8s 38ms/step - loss: 0.0505 - accuracy: 0.9814 - val_loss: 1.6130 - val_accuracy: 0.7301
Epoch 14/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0456 - accuracy: 0.9827 - val_loss: 1.6454 - val_accuracy: 0.7261
Epoch 15/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0452 - accuracy: 0.9832 - val_loss: 1.6877 - val_accuracy: 0.7301
Epoch 16/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0422 - accuracy: 0.9849 - val_loss: 1.9281 - val_accuracy: 0.7267
Epoch 17/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0412 - accuracy: 0.9832 - val_loss: 1.7817 - val_accuracy: 0.7222
Epoch 18/20
220/220 [==============================] - 9s 40ms/step - loss: 0.0581 - accuracy: 0.9751 - val_loss: 1.6876 - val_accuracy: 0.7324
Epoch 19/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0396 - accuracy: 0.9835 - val_loss: 1.9302 - val_accuracy: 0.7312
Epoch 20/20
220/220 [==============================] - 9s 39ms/step - loss: 0.0386 - accuracy: 0.9842 - val_loss: 1.9721 - val_accuracy: 0.7250
69/69 [==============================] - 2s 14ms/step - loss: 1.6991 - accuracy: 0.7364
Test Loss: 1.699123740196228
Test Accuracy: 0.7363636493682861
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_21&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_21 (Embedding)    (None, 50, 50)            1360900   
                                                                 
 bidirectional_21 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_21 (Dropout)        (None, 512)               0         
                                                                 
 dense_21 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 18ms/step - loss: 0.5958 - accuracy: 0.6793 - val_loss: 0.5214 - val_accuracy: 0.7375
Epoch 2/20
220/220 [==============================] - 3s 16ms/step - loss: 0.3808 - accuracy: 0.8312 - val_loss: 0.5374 - val_accuracy: 0.7500
Epoch 3/20
220/220 [==============================] - 3s 16ms/step - loss: 0.2799 - accuracy: 0.8839 - val_loss: 0.6287 - val_accuracy: 0.7392
Epoch 4/20
220/220 [==============================] - 3s 16ms/step - loss: 0.2638 - accuracy: 0.8932 - val_loss: 0.6653 - val_accuracy: 0.7278
Epoch 5/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1879 - accuracy: 0.9234 - val_loss: 0.7995 - val_accuracy: 0.7375
Epoch 6/20
220/220 [==============================] - 3s 16ms/step - loss: 0.1484 - accuracy: 0.9452 - val_loss: 0.9794 - val_accuracy: 0.7472
Epoch 7/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1158 - accuracy: 0.9554 - val_loss: 1.0784 - val_accuracy: 0.7347
Epoch 8/20
220/220 [==============================] - 4s 16ms/step - loss: 0.3317 - accuracy: 0.8704 - val_loss: 0.8929 - val_accuracy: 0.7295
Epoch 9/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1606 - accuracy: 0.9342 - val_loss: 1.0971 - val_accuracy: 0.7267
Epoch 10/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0991 - accuracy: 0.9615 - val_loss: 1.2192 - val_accuracy: 0.7261
Epoch 11/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0758 - accuracy: 0.9729 - val_loss: 1.3023 - val_accuracy: 0.7312
Epoch 12/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0730 - accuracy: 0.9713 - val_loss: 1.3526 - val_accuracy: 0.7256
Epoch 13/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0604 - accuracy: 0.9780 - val_loss: 1.5054 - val_accuracy: 0.7239
Epoch 14/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0506 - accuracy: 0.9812 - val_loss: 1.5356 - val_accuracy: 0.7199
Epoch 15/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0512 - accuracy: 0.9812 - val_loss: 1.6157 - val_accuracy: 0.7307
Epoch 16/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0525 - accuracy: 0.9795 - val_loss: 1.6151 - val_accuracy: 0.7261
Epoch 17/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0433 - accuracy: 0.9832 - val_loss: 1.8005 - val_accuracy: 0.7295
Epoch 18/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0518 - accuracy: 0.9795 - val_loss: 1.6568 - val_accuracy: 0.7347
Epoch 19/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 1.7280 - val_accuracy: 0.7381
Epoch 20/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0423 - accuracy: 0.9835 - val_loss: 1.6402 - val_accuracy: 0.7352
69/69 [==============================] - 1s 6ms/step - loss: 1.5313 - accuracy: 0.7450
Test Loss: 1.5313156843185425
Test Accuracy: 0.7450000047683716
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_22&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_22 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_22 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_22 (Dropout)        (None, 512)               0         
                                                                 
 dense_22 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 8s 28ms/step - loss: 0.5932 - accuracy: 0.6698 - val_loss: 0.5679 - val_accuracy: 0.6977
Epoch 2/20
220/220 [==============================] - 5s 25ms/step - loss: 0.3983 - accuracy: 0.8193 - val_loss: 0.5417 - val_accuracy: 0.7233
Epoch 3/20
220/220 [==============================] - 5s 25ms/step - loss: 0.3056 - accuracy: 0.8717 - val_loss: 0.5819 - val_accuracy: 0.7437
Epoch 4/20
220/220 [==============================] - 5s 25ms/step - loss: 0.2268 - accuracy: 0.9088 - val_loss: 0.6382 - val_accuracy: 0.7403
Epoch 5/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1652 - accuracy: 0.9378 - val_loss: 0.8656 - val_accuracy: 0.7455
Epoch 6/20
220/220 [==============================] - 5s 24ms/step - loss: 0.1315 - accuracy: 0.9510 - val_loss: 0.9903 - val_accuracy: 0.7426
Epoch 7/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1002 - accuracy: 0.9619 - val_loss: 1.1926 - val_accuracy: 0.7432
Epoch 8/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0806 - accuracy: 0.9690 - val_loss: 1.1628 - val_accuracy: 0.7392
Epoch 9/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0627 - accuracy: 0.9776 - val_loss: 1.3312 - val_accuracy: 0.7352
Epoch 10/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0624 - accuracy: 0.9758 - val_loss: 1.4493 - val_accuracy: 0.7352
Epoch 11/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0539 - accuracy: 0.9794 - val_loss: 1.5324 - val_accuracy: 0.7273
Epoch 12/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0484 - accuracy: 0.9808 - val_loss: 1.6222 - val_accuracy: 0.7358
Epoch 13/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0455 - accuracy: 0.9824 - val_loss: 1.6822 - val_accuracy: 0.7330
Epoch 14/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0412 - accuracy: 0.9835 - val_loss: 1.7257 - val_accuracy: 0.7244
Epoch 15/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0409 - accuracy: 0.9849 - val_loss: 1.7655 - val_accuracy: 0.7301
Epoch 16/20
220/220 [==============================] - 6s 26ms/step - loss: 0.0389 - accuracy: 0.9839 - val_loss: 1.8997 - val_accuracy: 0.7358
Epoch 17/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0375 - accuracy: 0.9852 - val_loss: 1.9412 - val_accuracy: 0.7250
Epoch 18/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0371 - accuracy: 0.9845 - val_loss: 2.0025 - val_accuracy: 0.7301
Epoch 19/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0367 - accuracy: 0.9842 - val_loss: 1.7612 - val_accuracy: 0.7256
Epoch 20/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0383 - accuracy: 0.9839 - val_loss: 1.9187 - val_accuracy: 0.7267
69/69 [==============================] - 1s 9ms/step - loss: 1.6459 - accuracy: 0.7459
Test Loss: 1.6458779573440552
Test Accuracy: 0.7459090948104858
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 1, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_23&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_23 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_23 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_23 (Dropout)        (None, 512)               0         
                                                                 
 dense_23 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 12s 45ms/step - loss: 0.5858 - accuracy: 0.6773 - val_loss: 0.5179 - val_accuracy: 0.7335
Epoch 2/20
220/220 [==============================] - 10s 44ms/step - loss: 0.4478 - accuracy: 0.8232 - val_loss: 0.5324 - val_accuracy: 0.7534
Epoch 3/20
220/220 [==============================] - 10s 45ms/step - loss: 0.3080 - accuracy: 0.8733 - val_loss: 0.5911 - val_accuracy: 0.7455
Epoch 4/20
220/220 [==============================] - 9s 43ms/step - loss: 0.2354 - accuracy: 0.9069 - val_loss: 0.6695 - val_accuracy: 0.7466
Epoch 5/20
220/220 [==============================] - 9s 43ms/step - loss: 0.1908 - accuracy: 0.9274 - val_loss: 0.7905 - val_accuracy: 0.7381
Epoch 6/20
220/220 [==============================] - 9s 42ms/step - loss: 0.1686 - accuracy: 0.9348 - val_loss: 0.8759 - val_accuracy: 0.7415
Epoch 7/20
220/220 [==============================] - 9s 43ms/step - loss: 0.1307 - accuracy: 0.9511 - val_loss: 1.0069 - val_accuracy: 0.7290
Epoch 8/20
220/220 [==============================] - 9s 43ms/step - loss: 0.1096 - accuracy: 0.9601 - val_loss: 1.1112 - val_accuracy: 0.7301
Epoch 9/20
220/220 [==============================] - 10s 43ms/step - loss: 0.0863 - accuracy: 0.9690 - val_loss: 1.2323 - val_accuracy: 0.7244
Epoch 10/20
220/220 [==============================] - 9s 43ms/step - loss: 0.0708 - accuracy: 0.9749 - val_loss: 1.4142 - val_accuracy: 0.7307
Epoch 11/20
220/220 [==============================] - 10s 43ms/step - loss: 0.0668 - accuracy: 0.9767 - val_loss: 1.4017 - val_accuracy: 0.7244
Epoch 12/20
220/220 [==============================] - 10s 44ms/step - loss: 0.0551 - accuracy: 0.9805 - val_loss: 1.5395 - val_accuracy: 0.7170
Epoch 13/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0582 - accuracy: 0.9791 - val_loss: 1.5476 - val_accuracy: 0.7256
Epoch 14/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0518 - accuracy: 0.9795 - val_loss: 1.6792 - val_accuracy: 0.7210
Epoch 15/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0458 - accuracy: 0.9822 - val_loss: 1.8121 - val_accuracy: 0.7284
Epoch 16/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0464 - accuracy: 0.9828 - val_loss: 1.7307 - val_accuracy: 0.7290
Epoch 17/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0428 - accuracy: 0.9835 - val_loss: 1.7702 - val_accuracy: 0.7210
Epoch 18/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0452 - accuracy: 0.9827 - val_loss: 1.8490 - val_accuracy: 0.7256
Epoch 19/20
220/220 [==============================] - 10s 44ms/step - loss: 0.0392 - accuracy: 0.9852 - val_loss: 1.8694 - val_accuracy: 0.7250
Epoch 20/20
220/220 [==============================] - 10s 45ms/step - loss: 0.0379 - accuracy: 0.9838 - val_loss: 1.9624 - val_accuracy: 0.7199
69/69 [==============================] - 2s 17ms/step - loss: 1.8115 - accuracy: 0.7286
Test Loss: 1.8115359544754028
Test Accuracy: 0.7286363840103149
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_24&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_24 (Embedding)    (None, 50, 50)            1360900   
                                                                 
 bidirectional_24 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_24 (Dropout)        (None, 512)               0         
                                                                 
 dense_24 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 20ms/step - loss: 0.5965 - accuracy: 0.6748 - val_loss: 0.5160 - val_accuracy: 0.7483
Epoch 2/20
220/220 [==============================] - 4s 16ms/step - loss: 0.3886 - accuracy: 0.8237 - val_loss: 0.5170 - val_accuracy: 0.7523
Epoch 3/20
220/220 [==============================] - 4s 16ms/step - loss: 0.2808 - accuracy: 0.8828 - val_loss: 0.6055 - val_accuracy: 0.7534
Epoch 4/20
220/220 [==============================] - 4s 16ms/step - loss: 0.2605 - accuracy: 0.8929 - val_loss: 0.6694 - val_accuracy: 0.7511
Epoch 5/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1853 - accuracy: 0.9264 - val_loss: 0.7579 - val_accuracy: 0.7415
Epoch 6/20
220/220 [==============================] - 3s 16ms/step - loss: 0.1506 - accuracy: 0.9442 - val_loss: 0.8865 - val_accuracy: 0.7432
Epoch 7/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1117 - accuracy: 0.9575 - val_loss: 1.0517 - val_accuracy: 0.7426
Epoch 8/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0914 - accuracy: 0.9652 - val_loss: 1.2702 - val_accuracy: 0.7318
Epoch 9/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0720 - accuracy: 0.9733 - val_loss: 1.2843 - val_accuracy: 0.7324
Epoch 10/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0684 - accuracy: 0.9750 - val_loss: 1.4549 - val_accuracy: 0.7330
Epoch 11/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0589 - accuracy: 0.9781 - val_loss: 1.5984 - val_accuracy: 0.7278
Epoch 12/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0510 - accuracy: 0.9804 - val_loss: 1.6752 - val_accuracy: 0.7301
Epoch 13/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0445 - accuracy: 0.9834 - val_loss: 1.7759 - val_accuracy: 0.7324
Epoch 14/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0414 - accuracy: 0.9845 - val_loss: 1.8803 - val_accuracy: 0.7284
Epoch 15/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0450 - accuracy: 0.9829 - val_loss: 1.5363 - val_accuracy: 0.7318
Epoch 16/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0536 - accuracy: 0.9790 - val_loss: 1.7466 - val_accuracy: 0.7267
Epoch 17/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0410 - accuracy: 0.9835 - val_loss: 1.7891 - val_accuracy: 0.7330
Epoch 18/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0387 - accuracy: 0.9849 - val_loss: 1.9673 - val_accuracy: 0.7261
Epoch 19/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0351 - accuracy: 0.9862 - val_loss: 2.0006 - val_accuracy: 0.7199
Epoch 20/20
220/220 [==============================] - 3s 16ms/step - loss: 0.0338 - accuracy: 0.9856 - val_loss: 2.1182 - val_accuracy: 0.7301
69/69 [==============================] - 1s 6ms/step - loss: 1.9158 - accuracy: 0.7373
Test Loss: 1.9158016443252563
Test Accuracy: 0.7372727394104004
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_25&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_25 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_25 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_25 (Dropout)        (None, 512)               0         
                                                                 
 dense_25 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 8s 27ms/step - loss: 0.5917 - accuracy: 0.6724 - val_loss: 0.5574 - val_accuracy: 0.7165
Epoch 2/20
220/220 [==============================] - 6s 25ms/step - loss: 0.3811 - accuracy: 0.8339 - val_loss: 0.5342 - val_accuracy: 0.7477
Epoch 3/20
220/220 [==============================] - 6s 25ms/step - loss: 0.2849 - accuracy: 0.8778 - val_loss: 0.6343 - val_accuracy: 0.7534
Epoch 4/20
220/220 [==============================] - 6s 26ms/step - loss: 0.2198 - accuracy: 0.9111 - val_loss: 0.6739 - val_accuracy: 0.7369
Epoch 5/20
220/220 [==============================] - 6s 25ms/step - loss: 0.1693 - accuracy: 0.9355 - val_loss: 0.8217 - val_accuracy: 0.7483
Epoch 6/20
220/220 [==============================] - 5s 25ms/step - loss: 0.1303 - accuracy: 0.9508 - val_loss: 0.9140 - val_accuracy: 0.7432
Epoch 7/20
220/220 [==============================] - 6s 26ms/step - loss: 0.0951 - accuracy: 0.9645 - val_loss: 1.1074 - val_accuracy: 0.7364
Epoch 8/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0836 - accuracy: 0.9680 - val_loss: 1.0951 - val_accuracy: 0.7278
Epoch 9/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0700 - accuracy: 0.9739 - val_loss: 1.3521 - val_accuracy: 0.7307
Epoch 10/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0588 - accuracy: 0.9793 - val_loss: 1.4783 - val_accuracy: 0.7267
Epoch 11/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0516 - accuracy: 0.9814 - val_loss: 1.5115 - val_accuracy: 0.7239
Epoch 12/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0491 - accuracy: 0.9808 - val_loss: 1.6787 - val_accuracy: 0.7165
Epoch 13/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0525 - accuracy: 0.9804 - val_loss: 1.6485 - val_accuracy: 0.7222
Epoch 14/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0511 - accuracy: 0.9805 - val_loss: 1.8132 - val_accuracy: 0.7222
Epoch 15/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0478 - accuracy: 0.9811 - val_loss: 1.8280 - val_accuracy: 0.7188
Epoch 16/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0419 - accuracy: 0.9838 - val_loss: 1.8343 - val_accuracy: 0.7159
Epoch 17/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0369 - accuracy: 0.9852 - val_loss: 2.1290 - val_accuracy: 0.7239
Epoch 18/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0382 - accuracy: 0.9845 - val_loss: 1.7604 - val_accuracy: 0.7205
Epoch 19/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0338 - accuracy: 0.9855 - val_loss: 1.9579 - val_accuracy: 0.7261
Epoch 20/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0339 - accuracy: 0.9854 - val_loss: 2.0459 - val_accuracy: 0.7261
69/69 [==============================] - 1s 9ms/step - loss: 1.8311 - accuracy: 0.7377
Test Loss: 1.8310697078704834
Test Accuracy: 0.7377272844314575
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 2, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_26&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_26 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_26 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_26 (Dropout)        (None, 512)               0         
                                                                 
 dense_26 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 12s 45ms/step - loss: 0.6130 - accuracy: 0.6601 - val_loss: 0.5371 - val_accuracy: 0.7125
Epoch 2/20
220/220 [==============================] - 9s 41ms/step - loss: 0.4458 - accuracy: 0.7901 - val_loss: 0.5481 - val_accuracy: 0.7159
Epoch 3/20
220/220 [==============================] - 9s 41ms/step - loss: 0.3120 - accuracy: 0.8622 - val_loss: 0.5625 - val_accuracy: 0.7330
Epoch 4/20
220/220 [==============================] - 9s 41ms/step - loss: 0.2323 - accuracy: 0.9017 - val_loss: 0.6714 - val_accuracy: 0.7381
Epoch 5/20
220/220 [==============================] - 10s 44ms/step - loss: 0.1802 - accuracy: 0.9267 - val_loss: 0.8072 - val_accuracy: 0.7409
Epoch 6/20
220/220 [==============================] - 9s 42ms/step - loss: 0.1324 - accuracy: 0.9496 - val_loss: 0.9214 - val_accuracy: 0.7477
Epoch 7/20
220/220 [==============================] - 9s 43ms/step - loss: 0.1066 - accuracy: 0.9606 - val_loss: 1.0221 - val_accuracy: 0.7352
Epoch 8/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0905 - accuracy: 0.9675 - val_loss: 1.1531 - val_accuracy: 0.7369
Epoch 9/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0686 - accuracy: 0.9751 - val_loss: 1.3453 - val_accuracy: 0.7347
Epoch 10/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0576 - accuracy: 0.9793 - val_loss: 1.4407 - val_accuracy: 0.7381
Epoch 11/20
220/220 [==============================] - 10s 43ms/step - loss: 0.0537 - accuracy: 0.9817 - val_loss: 1.4604 - val_accuracy: 0.7307
Epoch 12/20
220/220 [==============================] - 9s 41ms/step - loss: 0.0496 - accuracy: 0.9820 - val_loss: 1.5823 - val_accuracy: 0.7358
Epoch 13/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0441 - accuracy: 0.9839 - val_loss: 1.8193 - val_accuracy: 0.7352
Epoch 14/20
220/220 [==============================] - 9s 43ms/step - loss: 0.0453 - accuracy: 0.9828 - val_loss: 1.5999 - val_accuracy: 0.7347
Epoch 15/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0500 - accuracy: 0.9824 - val_loss: 1.5752 - val_accuracy: 0.7244
Epoch 16/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0423 - accuracy: 0.9832 - val_loss: 1.8679 - val_accuracy: 0.7307
Epoch 17/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 1.8766 - val_accuracy: 0.7318
Epoch 18/20
220/220 [==============================] - 9s 43ms/step - loss: 0.0372 - accuracy: 0.9848 - val_loss: 1.8548 - val_accuracy: 0.7358
Epoch 19/20
220/220 [==============================] - 10s 43ms/step - loss: 0.0358 - accuracy: 0.9848 - val_loss: 2.0321 - val_accuracy: 0.7261
Epoch 20/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0388 - accuracy: 0.9841 - val_loss: 1.8974 - val_accuracy: 0.7318
69/69 [==============================] - 2s 15ms/step - loss: 1.7548 - accuracy: 0.7327
Test Loss: 1.7547560930252075
Test Accuracy: 0.7327272891998291
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 50}
Model: &quot;sequential_27&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_27 (Embedding)    (None, 50, 50)            1360900   
                                                                 
 bidirectional_27 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_27 (Dropout)        (None, 512)               0         
                                                                 
 dense_27 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 6s 19ms/step - loss: 0.5914 - accuracy: 0.6748 - val_loss: 0.5149 - val_accuracy: 0.7369
Epoch 2/20
220/220 [==============================] - 3s 16ms/step - loss: 0.3931 - accuracy: 0.8247 - val_loss: 0.5221 - val_accuracy: 0.7420
Epoch 3/20
220/220 [==============================] - 4s 16ms/step - loss: 0.2864 - accuracy: 0.8806 - val_loss: 0.5873 - val_accuracy: 0.7568
Epoch 4/20
220/220 [==============================] - 4s 18ms/step - loss: 0.2252 - accuracy: 0.9084 - val_loss: 0.7005 - val_accuracy: 0.7449
Epoch 5/20
220/220 [==============================] - 4s 17ms/step - loss: 0.1754 - accuracy: 0.9321 - val_loss: 0.8281 - val_accuracy: 0.7432
Epoch 6/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1405 - accuracy: 0.9477 - val_loss: 0.9425 - val_accuracy: 0.7443
Epoch 7/20
220/220 [==============================] - 4s 16ms/step - loss: 0.1231 - accuracy: 0.9523 - val_loss: 0.9669 - val_accuracy: 0.7386
Epoch 8/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0934 - accuracy: 0.9649 - val_loss: 1.0734 - val_accuracy: 0.7330
Epoch 9/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0807 - accuracy: 0.9707 - val_loss: 1.2477 - val_accuracy: 0.7409
Epoch 10/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0609 - accuracy: 0.9776 - val_loss: 1.5856 - val_accuracy: 0.7386
Epoch 11/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0569 - accuracy: 0.9795 - val_loss: 1.3957 - val_accuracy: 0.7364
Epoch 12/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0535 - accuracy: 0.9778 - val_loss: 1.4791 - val_accuracy: 0.7222
Epoch 13/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0509 - accuracy: 0.9804 - val_loss: 1.5127 - val_accuracy: 0.7341
Epoch 14/20
220/220 [==============================] - 4s 16ms/step - loss: 0.0451 - accuracy: 0.9821 - val_loss: 1.7807 - val_accuracy: 0.7330
Epoch 15/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0452 - accuracy: 0.9837 - val_loss: 1.6499 - val_accuracy: 0.7267
Epoch 16/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0418 - accuracy: 0.9842 - val_loss: 1.8121 - val_accuracy: 0.7256
Epoch 17/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0395 - accuracy: 0.9851 - val_loss: 1.7421 - val_accuracy: 0.7239
Epoch 18/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0361 - accuracy: 0.9861 - val_loss: 1.8714 - val_accuracy: 0.7216
Epoch 19/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0355 - accuracy: 0.9851 - val_loss: 2.0327 - val_accuracy: 0.7216
Epoch 20/20
220/220 [==============================] - 4s 17ms/step - loss: 0.0323 - accuracy: 0.9871 - val_loss: 2.1017 - val_accuracy: 0.7261
69/69 [==============================] - 1s 6ms/step - loss: 1.7973 - accuracy: 0.7518
Test Loss: 1.7973421812057495
Test Accuracy: 0.7518181800842285
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 100}
Model: &quot;sequential_28&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_28 (Embedding)    (None, 100, 50)           1360900   
                                                                 
 bidirectional_28 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_28 (Dropout)        (None, 512)               0         
                                                                 
 dense_28 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 8s 28ms/step - loss: 0.5935 - accuracy: 0.6724 - val_loss: 0.5353 - val_accuracy: 0.7295
Epoch 2/20
220/220 [==============================] - 6s 25ms/step - loss: 0.4008 - accuracy: 0.8225 - val_loss: 0.5374 - val_accuracy: 0.7540
Epoch 3/20
220/220 [==============================] - 8s 35ms/step - loss: 0.2867 - accuracy: 0.8843 - val_loss: 0.5985 - val_accuracy: 0.7631
Epoch 4/20
220/220 [==============================] - 7s 32ms/step - loss: 0.2169 - accuracy: 0.9149 - val_loss: 0.6773 - val_accuracy: 0.7528
Epoch 5/20
220/220 [==============================] - 6s 25ms/step - loss: 0.1630 - accuracy: 0.9392 - val_loss: 0.8223 - val_accuracy: 0.7415
Epoch 6/20
220/220 [==============================] - 6s 25ms/step - loss: 0.1196 - accuracy: 0.9557 - val_loss: 0.9537 - val_accuracy: 0.7420
Epoch 7/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0971 - accuracy: 0.9641 - val_loss: 1.1735 - val_accuracy: 0.7460
Epoch 8/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0848 - accuracy: 0.9690 - val_loss: 1.1530 - val_accuracy: 0.7358
Epoch 9/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0700 - accuracy: 0.9756 - val_loss: 1.3909 - val_accuracy: 0.7347
Epoch 10/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0615 - accuracy: 0.9767 - val_loss: 1.3634 - val_accuracy: 0.7250
Epoch 11/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0719 - accuracy: 0.9731 - val_loss: 1.3521 - val_accuracy: 0.7335
Epoch 12/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0523 - accuracy: 0.9801 - val_loss: 1.6132 - val_accuracy: 0.7324
Epoch 13/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0454 - accuracy: 0.9827 - val_loss: 1.7067 - val_accuracy: 0.7318
Epoch 14/20
220/220 [==============================] - 6s 26ms/step - loss: 0.0423 - accuracy: 0.9832 - val_loss: 1.6959 - val_accuracy: 0.7330
Epoch 15/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0474 - accuracy: 0.9815 - val_loss: 1.7320 - val_accuracy: 0.7267
Epoch 16/20
220/220 [==============================] - 6s 25ms/step - loss: 0.0867 - accuracy: 0.9690 - val_loss: 1.5835 - val_accuracy: 0.7205
Epoch 17/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0447 - accuracy: 0.9834 - val_loss: 1.6913 - val_accuracy: 0.7205
Epoch 18/20
220/220 [==============================] - 5s 24ms/step - loss: 0.0407 - accuracy: 0.9838 - val_loss: 1.7468 - val_accuracy: 0.7233
Epoch 19/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0403 - accuracy: 0.9847 - val_loss: 1.5962 - val_accuracy: 0.7284
Epoch 20/20
220/220 [==============================] - 5s 25ms/step - loss: 0.0366 - accuracy: 0.9849 - val_loss: 1.8343 - val_accuracy: 0.7250
69/69 [==============================] - 2s 10ms/step - loss: 1.5839 - accuracy: 0.7477
Test Loss: 1.5839117765426636
Test Accuracy: 0.7477272748947144
Entrenando modelo con parámetros: {&#39;batch_size&#39;: 32, &#39;dropout_rate&#39;: 0.2, &#39;embedding_dim&#39;: 100, &#39;epochs&#39;: 20, &#39;learning_rate&#39;: 0.001, &#39;lstm_units&#39;: 256, &#39;num_lstm_layers&#39;: 3, &#39;sequence_length&#39;: 200}
Model: &quot;sequential_29&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_29 (Embedding)    (None, 200, 50)           1360900   
                                                                 
 bidirectional_29 (Bidirecti  (None, 512)              628736    
 onal)                                                           
                                                                 
 dropout_29 (Dropout)        (None, 512)               0         
                                                                 
 dense_29 (Dense)            (None, 1)                 513       
                                                                 
=================================================================
Total params: 1,990,149
Trainable params: 1,990,149
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
220/220 [==============================] - 12s 46ms/step - loss: 0.5956 - accuracy: 0.6725 - val_loss: 0.5150 - val_accuracy: 0.7324
Epoch 2/20
220/220 [==============================] - 10s 43ms/step - loss: 0.3816 - accuracy: 0.8316 - val_loss: 0.5418 - val_accuracy: 0.7534
Epoch 3/20
220/220 [==============================] - 9s 42ms/step - loss: 0.2793 - accuracy: 0.8833 - val_loss: 0.5928 - val_accuracy: 0.7551
Epoch 4/20
220/220 [==============================] - 9s 42ms/step - loss: 0.2311 - accuracy: 0.9069 - val_loss: 0.6314 - val_accuracy: 0.7449
Epoch 5/20
220/220 [==============================] - 9s 42ms/step - loss: 0.1833 - accuracy: 0.9314 - val_loss: 0.7735 - val_accuracy: 0.7392
Epoch 6/20
220/220 [==============================] - 9s 41ms/step - loss: 0.1548 - accuracy: 0.9427 - val_loss: 0.9086 - val_accuracy: 0.7398
Epoch 7/20
220/220 [==============================] - 9s 42ms/step - loss: 0.1412 - accuracy: 0.9491 - val_loss: 1.0340 - val_accuracy: 0.7318
Epoch 8/20
220/220 [==============================] - 9s 42ms/step - loss: 0.1113 - accuracy: 0.9612 - val_loss: 1.0955 - val_accuracy: 0.7324
Epoch 9/20
220/220 [==============================] - 10s 44ms/step - loss: 0.0960 - accuracy: 0.9639 - val_loss: 1.1241 - val_accuracy: 0.7312
Epoch 10/20
220/220 [==============================] - 9s 42ms/step - loss: 0.0783 - accuracy: 0.9713 - val_loss: 1.2974 - val_accuracy: 0.7324
Epoch 11/20
 68/220 [========&gt;.....................] - ETA: 6s - loss: 0.0594 - accuracy: 0.9747
</pre></div>
</div>
</div>
</div>
<p>Se escogen los mejores parámetros</p>
<p>Con un <code class="docutils literal notranslate"><span class="pre">Test</span> <span class="pre">Accuracy</span> <span class="pre">de</span> <span class="pre">0.7514</span> <span class="pre">(75.14%)</span></code>, se escogen:</p>
<ul class="simple">
<li><p>lstm_units:	64</p></li>
<li><p>num_lstm_layers:	1</p></li>
<li><p>dropout_rate:	0.3</p></li>
<li><p>learning_rate:	0.001</p></li>
<li><p>batch_size:	32</p></li>
<li><p>sequence_length:	200</p></li>
<li><p>epochs:	20</p></li>
<li><p>embedding_dim:	100</p></li>
</ul>
</section>
<section id="modelo-final-con-los-mejores-parametros">
<h2>Modelo final con los mejores parámetros<a class="headerlink" href="#modelo-final-con-los-mejores-parametros" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lstm_units</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_lstm_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">150</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Tokenización</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Codificación de etiquetas</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="n">output_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                    <span class="n">input_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">lstm_units</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_2 (Embedding)     (None, 150, 150)          3580650   
                                                                 
 bidirectional_2 (Bidirectio  (None, 64)               46848     
 nal)                                                            
                                                                 
 dropout_2 (Dropout)         (None, 64)                0         
                                                                 
 dense_2 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 3,627,563
Trainable params: 3,627,563
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
220/220 [==============================] - 11s 34ms/step - loss: 0.5898 - accuracy: 0.6750 - val_loss: 0.5189 - val_accuracy: 0.7290
Epoch 2/20
220/220 [==============================] - 7s 33ms/step - loss: 0.3691 - accuracy: 0.8400 - val_loss: 0.5308 - val_accuracy: 0.7472
Epoch 3/20
220/220 [==============================] - 7s 30ms/step - loss: 0.2735 - accuracy: 0.8893 - val_loss: 0.6206 - val_accuracy: 0.7523
Epoch 4/20
220/220 [==============================] - 6s 28ms/step - loss: 0.2025 - accuracy: 0.9213 - val_loss: 0.7073 - val_accuracy: 0.7437
Epoch 5/20
220/220 [==============================] - 6s 28ms/step - loss: 0.1471 - accuracy: 0.9471 - val_loss: 0.8297 - val_accuracy: 0.7420
Epoch 6/20
220/220 [==============================] - 6s 27ms/step - loss: 0.1150 - accuracy: 0.9574 - val_loss: 0.9423 - val_accuracy: 0.7420
Epoch 7/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0965 - accuracy: 0.9641 - val_loss: 0.9748 - val_accuracy: 0.7278
Epoch 8/20
220/220 [==============================] - 6s 27ms/step - loss: 0.0813 - accuracy: 0.9716 - val_loss: 1.1648 - val_accuracy: 0.7318
Epoch 9/20
220/220 [==============================] - 6s 27ms/step - loss: 0.0715 - accuracy: 0.9712 - val_loss: 1.2096 - val_accuracy: 0.7386
Epoch 10/20
220/220 [==============================] - 6s 29ms/step - loss: 0.0595 - accuracy: 0.9773 - val_loss: 1.3190 - val_accuracy: 0.7352
Epoch 11/20
220/220 [==============================] - 6s 29ms/step - loss: 0.0532 - accuracy: 0.9785 - val_loss: 1.4029 - val_accuracy: 0.7352
Epoch 12/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0495 - accuracy: 0.9800 - val_loss: 1.4506 - val_accuracy: 0.7244
Epoch 13/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0466 - accuracy: 0.9797 - val_loss: 1.5109 - val_accuracy: 0.7341
Epoch 14/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0423 - accuracy: 0.9810 - val_loss: 1.5776 - val_accuracy: 0.7295
Epoch 15/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0513 - accuracy: 0.9784 - val_loss: 1.5511 - val_accuracy: 0.7261
Epoch 16/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0546 - accuracy: 0.9768 - val_loss: 1.5444 - val_accuracy: 0.7312
Epoch 17/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0427 - accuracy: 0.9818 - val_loss: 1.6900 - val_accuracy: 0.7256
Epoch 18/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0398 - accuracy: 0.9821 - val_loss: 1.7039 - val_accuracy: 0.7210
Epoch 19/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0342 - accuracy: 0.9842 - val_loss: 1.8659 - val_accuracy: 0.7244
Epoch 20/20
220/220 [==============================] - 6s 28ms/step - loss: 0.0339 - accuracy: 0.9848 - val_loss: 1.9010 - val_accuracy: 0.7222
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Test Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>69/69 [==============================] - 1s 14ms/step - loss: 1.6857 - accuracy: 0.7573

Test Loss: 1.6857
Test Accuracy: 0.7573
</pre></div>
</div>
</div>
</div>
<p>El modelo alcanzó una precisión final en el conjunto de entrenamiento cercana al 99%, mientras que en el conjunto de validación esta métrica se estabilizó en torno al 73%. Esta diferencia marcada evidencia una alta capacidad del modelo para aprender los patrones presentes en los datos con los que fue entrenado, pero una limitada habilidad para generalizar dicho conocimiento a nuevos ejemplos. Además, se observa una tendencia en la cual el error de validación no solo se mantiene alto, sino que empeora a medida que el entrenamiento avanza. Esto indica que el modelo continúa ajustándose a detalles específicos del entrenamiento que no son relevantes para la generalización, lo cual sugiere la presencia de sobreajuste.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Entrenamiento&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validación&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Pérdida durante entrenamiento&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Épocas&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Entrenamiento&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validación&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Precisión durante entrenamiento&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Épocas&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a4a3c3f1aa69852e49f93717a211b7e1354e2fd97fdbefaa810e97c3ad954eeb.png" src="_images/a4a3c3f1aa69852e49f93717a211b7e1354e2fd97fdbefaa810e97c3ad954eeb.png" />
</div>
</div>
<section id="metricas-de-prediccion">
<h3>Métricas de Predicción<a class="headerlink" href="#metricas-de-prediccion" title="Link to this heading">#</a></h3>
<p>El modelo entrenado alcanza una precisión global (accuracy) del 76%, lo que indica que aproximadamente tres de cada cuatro predicciones realizadas coinciden con la etiqueta real. Este valor, aunque no extremadamente alto, sí representa un desempeño aceptable considerando la naturaleza del problema y el tipo de datos utilizados. Más allá del valor general de precisión, es clave examinar las métricas específicas por clase para tener una comprensión más profunda del comportamiento del modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred_probs</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>69/69 [==============================] - 1s 13ms/step
</pre></div>
</div>
</div>
</div>
<p>El modelo alcanza una precisión global (accuracy) de 76%, lo que indica que clasifica correctamente tres de cada cuatro instancias. Este valor, aunque no sobresaliente, sí representa un desempeño competitivo para tareas de clasificación de texto como análisis de sentimiento, especialmente cuando se mantiene balance entre clases. Los valores de precision, recall y f1-score están distribuidos de forma relativamente equilibrada, tanto entre clases como en sus promedios.</p>
<ol class="arabic simple">
<li><p>Clase Negativo:</p></li>
</ol>
<ul class="simple">
<li><p>Precision: 0.77 = el modelo es bastante certero cuando predice que un ejemplo es negativo.</p></li>
<li><p>Recall: 0.72 = logra recuperar el 72% de todos los casos negativos reales.</p></li>
<li><p>F1-score: 0.74 = equilibrio aceptable entre ambas métricas.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Clase Positivo:</p></li>
</ol>
<ul class="simple">
<li><p>Precision: 0.75= ligeramente inferior, pero aún dentro de un rango óptimo.</p></li>
<li><p>Recall: 0.79 = destaca por su capacidad de detección de casos positivos.</p></li>
<li><p>F1-score: 0.77 = sólido, lo que indica consistencia en el rendimiento.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Reporte de clasificación:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Reporte de clasificación:
              precision    recall  f1-score   support

    Negativo       0.77      0.72      0.74      1075
    Positivo       0.75      0.79      0.77      1125

    accuracy                           0.76      2200
   macro avg       0.76      0.76      0.76      2200
weighted avg       0.76      0.76      0.76      2200
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_probs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">AUC: </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.8049
</pre></div>
</div>
</div>
</div>
<div class="admonition-curva-roc-y-auc admonition">
<p class="admonition-title">Curva ROC y AUC</p>
<p>La curva ROC (Receiver Operating Characteristic) representa la relación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) a diferentes umbrales de decisión. El área bajo esta curva, conocida como AUC (Area Under the Curve), es una métrica robusta para evaluar el desempeño general de un modelo de clasificación binaria, especialmente en escenarios donde el balance entre clases puede influir en la precisión simple.</p>
</div>
<p>En este caso, el valor 0.8049 indica un desempeño sólidamente bueno, lo que significa que, ante dos ejemplos aleatorios —uno positivo y uno negativo— el modelo tiene un 80.49% de probabilidad de asignar una puntuación mayor al positivo. Este valor está por encima del umbral de 0.8, que suele marcar un punto de calidad en la mayoría de problemas de clasificación. Implica que el modelo no solo tiene un buen rendimiento en un único punto de corte (como cuando se evalúa con accuracy a 0.5), sino que se comporta bien en un rango amplio de umbrales.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_probs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;AUC = </span><span class="si">{</span><span class="n">auc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Positive Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ROC Curve&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c34f3898d5953ea6745ed18ae319f5b035fc2a8a033b57e31c9f3166abe8acd5.png" src="_images/c34f3898d5953ea6745ed18ae319f5b035fc2a8a033b57e31c9f3166abe8acd5.png" />
</div>
</div>
<ul class="simple">
<li><p>775 verdaderos negativos (TN): ejemplos negativos correctamente clasificados.</p></li>
<li><p>300 falsos positivos (FP): ejemplos negativos clasificados incorrectamente como positivos.</p></li>
<li><p>891 verdaderos positivos (TP): ejemplos positivos correctamente clasificados.</p></li>
<li><p>234 falsos negativos (FN): ejemplos positivos que fueron clasificados como negativos.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Matriz de Confusión&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicción&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Real&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2ed628ad6782886d5c1f7d8791e8a538616b8d0c6ce25239b8bcb41e4c06ab40.png" src="_images/2ed628ad6782886d5c1f7d8791e8a538616b8d0c6ce25239b8bcb41e4c06ab40.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lstm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><strong>LSTM: Long Short-Term Memory</strong></p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#librerias-y-paquetes-a-utilizar">Librerías y paquetes a utilizar</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-bidireccional-long-short-term-memory-lstm">Modelo Bidireccional Long Short Term Memory (LSTM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cargue-de-datos">Cargue de datos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametros-del-modelo">Parámetros del modelo</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hiperparametrizacion-para-bilstm">Hiperparametrización para BiLSTM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-final-con-los-mejores-parametros">Modelo final con los mejores parámetros</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metricas-de-prediccion">Métricas de Predicción</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kanery Camargo. Emanuel Carbonell. Henry Saenz.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>